<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Claude writing a Telegram Bot, 3rd try | ~stesie's musings</title>
<meta name=keywords content="AI,Claude,Telegram,Image Recognition"><meta name=description content="This is the third part of the mini series on writing Telegram bots to read my gas meter using Claude.
‚ÑπÔ∏è This post belongs to a mini series (I&rsquo;m aiming for three posts in a row), where I try prompting Claude with different levels of verbosity and compare the results:
In Claude writing a Telegram Bot, 1st try I&rsquo;m using an elaborate, step-by-step prompting strategy with Haiku model In Claude writing a Telegram Bot, 2nd try I aim for a detailed one-shot prompt In Claude writing a Telegram Bot, 3rd try I try a rather short one-shot prompt In Other LLMs writing a Telegram Bot I quickly try DeepSeek and some more LLMs Like in the first round, I will once more show the full dialog I&rsquo;ve used to let it generate the implementation."><meta name=author content><link rel=canonical href=https://stefansiegl.de/2025/02/claude-writing-a-telegram-bot-3rd-try/><link crossorigin=anonymous href=/assets/css/stylesheet.0c89971808195fcfba02b00128bb5209e165ff89d76c2f0d04eaefef9e1bd82a.css integrity="sha256-DImXGAgZX8+6ArABKLtSCeFl/4nXbC8NBOrv754b2Co=" rel="preload stylesheet" as=style><link rel=icon href=https://stefansiegl.de/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://stefansiegl.de/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://stefansiegl.de/favicon-32x32.png><link rel=apple-touch-icon href=https://stefansiegl.de/apple-touch-icon.png><link rel=mask-icon href=https://stefansiegl.de/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://stefansiegl.de/2025/02/claude-writing-a-telegram-bot-3rd-try/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Claude writing a Telegram Bot, 3rd try"><meta property="og:description" content="This is the third part of the mini series on writing Telegram bots to read my gas meter using Claude.
‚ÑπÔ∏è This post belongs to a mini series (I&rsquo;m aiming for three posts in a row), where I try prompting Claude with different levels of verbosity and compare the results:
In Claude writing a Telegram Bot, 1st try I&rsquo;m using an elaborate, step-by-step prompting strategy with Haiku model In Claude writing a Telegram Bot, 2nd try I aim for a detailed one-shot prompt In Claude writing a Telegram Bot, 3rd try I try a rather short one-shot prompt In Other LLMs writing a Telegram Bot I quickly try DeepSeek and some more LLMs Like in the first round, I will once more show the full dialog I&rsquo;ve used to let it generate the implementation."><meta property="og:type" content="article"><meta property="og:url" content="https://stefansiegl.de/2025/02/claude-writing-a-telegram-bot-3rd-try/"><meta property="article:section" content="pages"><meta property="article:published_time" content="2025-02-01T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-02T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Claude writing a Telegram Bot, 3rd try"><meta name=twitter:description content="This is the third part of the mini series on writing Telegram bots to read my gas meter using Claude.
‚ÑπÔ∏è This post belongs to a mini series (I&rsquo;m aiming for three posts in a row), where I try prompting Claude with different levels of verbosity and compare the results:
In Claude writing a Telegram Bot, 1st try I&rsquo;m using an elaborate, step-by-step prompting strategy with Haiku model In Claude writing a Telegram Bot, 2nd try I aim for a detailed one-shot prompt In Claude writing a Telegram Bot, 3rd try I try a rather short one-shot prompt In Other LLMs writing a Telegram Bot I quickly try DeepSeek and some more LLMs Like in the first round, I will once more show the full dialog I&rsquo;ve used to let it generate the implementation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Pages","item":"https://stefansiegl.de/pages/"},{"@type":"ListItem","position":2,"name":"Claude writing a Telegram Bot, 3rd try","item":"https://stefansiegl.de/2025/02/claude-writing-a-telegram-bot-3rd-try/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Claude writing a Telegram Bot, 3rd try","name":"Claude writing a Telegram Bot, 3rd try","description":"This is the third part of the mini series on writing Telegram bots to read my gas meter using Claude.\n‚ÑπÔ∏è This post belongs to a mini series (I\u0026rsquo;m aiming for three posts in a row), where I try prompting Claude with different levels of verbosity and compare the results:\nIn Claude writing a Telegram Bot, 1st try I\u0026rsquo;m using an elaborate, step-by-step prompting strategy with Haiku model In Claude writing a Telegram Bot, 2nd try I aim for a detailed one-shot prompt In Claude writing a Telegram Bot, 3rd try I try a rather short one-shot prompt In Other LLMs writing a Telegram Bot I quickly try DeepSeek and some more LLMs Like in the first round, I will once more show the full dialog I\u0026rsquo;ve used to let it generate the implementation.","keywords":["AI","Claude","Telegram","Image Recognition"],"articleBody":"This is the third part of the mini series on writing Telegram bots to read my gas meter using Claude.\n‚ÑπÔ∏è This post belongs to a mini series (I‚Äôm aiming for three posts in a row), where I try prompting Claude with different levels of verbosity and compare the results:\nIn Claude writing a Telegram Bot, 1st try I‚Äôm using an elaborate, step-by-step prompting strategy with Haiku model In Claude writing a Telegram Bot, 2nd try I aim for a detailed one-shot prompt In Claude writing a Telegram Bot, 3rd try I try a rather short one-shot prompt In Other LLMs writing a Telegram Bot I quickly try DeepSeek and some more LLMs Like in the first round, I will once more show the full dialog I‚Äôve used to let it generate the implementation. Then follow up with a discussion of the code and some final thoughts.\nThis is once more run against Claude 3.5 Haiku, so it‚Äôs comparable to the previous two posts. This time I omitted asking for intermediate steps like, scale the image to reduce the number of tokens used, and which prompt to use.\nCreate a telegram chatbot in JavaScript. I want to take a photo of a gas meter. Upload to OpenAI API to read the gas meter. The number has five digits before comma, and three decimal places. Then store the value to a postgres database table and return it in the chat. Here‚Äôs a breakdown of the implementation:\nTelegram Bot Setup: Uses telegraf for handling Telegram bot interactions Responds to /start command Handles photo uploads Image Processing: Downloads the uploaded photo Uses OpenAI‚Äôs GPT-4 Vision API to read the meter number Validates the reading format (5 digits, 3 decimal places) Database Storage: Uses PostgreSQL to store meter readings Includes user ID, reading, and timestamp Includes a method to set up the database table Error Handling: Robust error handling for file downloads, API calls, and database operations Prerequisites and Setup:\nInstall Node.js (version 16+) Create a Telegram bot via BotFather Get an OpenAI API key Set up a PostgreSQL database Install dependencies: npm install Create a .env file with your credentials Run the bot: npm start Notes and Recommendations:\nEnsure you have the latest versions of the dependencies Implement proper error logging in production Consider rate limiting and additional security measures The bot currently assumes a specific meter reading format Would you like me to elaborate on any part of the implementation or explain how to set up and deploy this bot?\nThis is the full code, that it spat out on the first shot\nCode Review const TelegramBot = require('node-telegram-bot-api'); const { Telegraf } = require('telegraf'); const { createClient } = require('@supabase/supabase-js'); const { OpenAI } = require('openai'); const multer = require('multer'); const axios = require('axios'); const fs = require('fs'); const pg = require('pg'); Already the initial imports are weird. So in it‚Äôs summary comment it even mentioned, that it‚Äôs using telegraf as a dependency, to implement the chat bot. However it also imported node-telegram-bot-api ‚Ä¶ which it doesn‚Äôt use at all.\nLikewise for the @supabase/supabase-js import. I mean, it‚Äôs an interesting thought to use Supabase (which is an open source Firebase alternative, that‚Äôs actually offering Postgres databases) ‚Ä¶ but it‚Äôs neither what I asked for nor is it actually using it in it‚Äôs own code later on.\nFurthermore it‚Äôs importing multer and even configured it in the constructor:\n// Multer for handling file uploads this.upload = multer({ dest: 'uploads/', limits: { fileSize: 5 * 1024 * 1024 } // 5MB file size limit }); ‚Ä¶ but afterward it‚Äôs not using it !?\nA nice touch, that wasn‚Äôt present on the first two iterations is this:\n// Start command handler this.bot.command('start', (ctx) =\u003e { ctx.reply('Welcome! Send me a photo of your gas meter, and I\\'ll read and store the reading.'); }); ‚Ä¶ I never asked for it, nor thought of it ‚Ä¶ but it‚Äôs nice offering some ‚Äúcall to action‚Äù. Obviously it‚Äôs not really needed, given that the bot is only a personal one and I of course know how to use it. But from a product/project point of view it makes sense.\nLike the previous versions also it‚Äôs a bit lax on assumptions, where it can put temporary files:\nasync downloadFile(filePath) { const fileUrl = `https://api.telegram.org/file/bot${this.botToken}/${filePath}`; const localFilePath = `uploads/${Date.now()}_meter.jpg`; It simply assumes, that there‚Äôs a uploads/ folder, and furthermore, that it‚Äôs writable.\nOnce more it‚Äôs not using random names for the temp file and/or check if it‚Äôs accidentally overwriting stuff. It just writes.\n‚Ä¶ and as mentioned on the previous post, after all there‚Äôs no reason at all, to create a temporary file in the first place. The data may just be stored into a Buffer directly.\nFurthermore, in case of exceptions occuring while accessing the LLM and/or the database, it‚Äôll fail to delete the temp file:\n// Reply to user ctx.reply(`Meter reading recorded: ${meterReading}`); // Clean up uploaded file fs.unlinkSync(filePath); } catch (error) { console.error('Error processing meter reading:', error); ctx.reply('Sorry, I couldn\\'t process the meter reading. Please try again.'); } ‚Ä¶ it even thought of providing a catch block, but then simply forgot about it. Well, on the other hand, one might argue that this is intentional, given that it might aid debugging ü§∑\nI like that it was able to come up with a meaningful prompt for GPT\n{ role: \"user\", content: [ { type: \"text\", text: \"Read the number on this gas meter. Ensure it's a 5-digit number before the comma and 3 decimal places.\" } // ... ] } ‚Ä¶ so it can mimic the standpoint of an entity of it‚Äôs own, asking GPT as a third party.\nHowever it doesn‚Äôt consider the freedom in ways of answering it leaves to the GPT. Given this prompt GPT will never just return the pure number. It‚Äôll always at least prefix something like ‚ÄúThe gas meter reading is ‚Ä¶‚Äù.\nTherefore it‚Äôs a nice touch, that it even comes up with a regex, asserting the format:\nvalidateMeterReading(reading) { // Regex to match 5 digits before comma, 3 decimal places const meterReadingRegex = /^\\d{5}\\.\\d{3}$/; return meterReadingRegex.test(reading); } ‚Ä¶ but that‚Äôs going to fail in all the cases, given that it didn‚Äôt tell the LLM, that it will make this assertion.\nWhat‚Äôs actually confusing, is that it just provided some example usage at the end of the file:\n// Export for use module.exports = GasMeterBot; // Example instantiation // const gasMeterBot = new GasMeterBot(); // gasMeterBot.setupDatabase(); ‚Ä¶ so it‚Äôs actually just creating a module ‚Ä¶ that it doesn‚Äôt instantiate anywhere. Which after all is in contrast to the ‚ÄúSetup steps‚Äù it brought forward, where it told to run npm start. But given that this instruction is commented out, it just will do nothing if started ‚Ä¶\nComparison with Sonnet I‚Äôve also quickly tried exactly the same prompt with Claude 3.5 Sonnet, which came up with the following prompt:\n{ type: \"text\", text: \"Read the gas meter value in this image. Return only the number with 5 digits before decimal and 3 after. Format: XXXXX.XXX\" } ‚Ä¶ where it clearly tells GPT which output it expects.\nIt also refrained from depending on other modules, which it wasn‚Äôt using.\nNeither the temporary file handling issues were present in that code, it just directly converted the ArrayBuffer into a Buffer, using Buffer.from.\nConclusion I think it‚Äôs fair to say that Claude 3.5 Haiku wasn‚Äôt able to properly cope with the given task. While it does provide a foundation to build upon, it definitely takes a human to address the issues at hand (either fixing directly, or providing further prompts to drive it forward). On the other hand it shows, that quite like you would reason about intermediate steps with (lesser experienced) human developers, you need to have this ‚Äúthinking together‚Äù phase with the LLM as well.\nThe quick comparison with Claude 3.5 Sonnet shows, that that one is far more capable and could handle even this task easily.\n‚ÑπÔ∏è There‚Äôs a follow-up to this article, where I tried the prompt from above against some other popular LLMs from DeepSeek, Meta \u0026 OpenAI: Other LLMs writing a Telegram Bot ","wordCount":"1353","inLanguage":"en","datePublished":"2025-02-01T00:00:00Z","dateModified":"2025-02-02T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://stefansiegl.de/2025/02/claude-writing-a-telegram-bot-3rd-try/"},"publisher":{"@type":"Organization","name":"~stesie's musings","logo":{"@type":"ImageObject","url":"https://stefansiegl.de/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://stefansiegl.de/ accesskey=h title="~stesie's musings (Alt + H)">~stesie's musings</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://stefansiegl.de/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://stefansiegl.de/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://stefansiegl.de/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://stefansiegl.de/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Claude writing a Telegram Bot, 3rd try</h1><div class=post-meta>Status: üåø Budding&nbsp;¬∑&nbsp;Planted:&nbsp;<span title='2025-02-01 00:00:00 +0000 UTC'>Feb 1, 2025</span>&nbsp;¬∑&nbsp;Last tended:&nbsp;<span title='2025-02-02 00:00:00 +0000 UTC'>Feb 2, 2025</span>&nbsp;¬∑&nbsp;7 min</div></header><div class=post-content><p>This is the third part of the mini series on writing Telegram bots to read my gas meter using Claude.</p><div class="admonition note"><div class=admonition-icon>‚ÑπÔ∏è</div><div class=admonition-body><p>This post belongs to a mini series (I&rsquo;m aiming for three posts in a row), where I try prompting Claude with different levels of verbosity and compare the results:</p><ul><li>In <a href=https://stefansiegl.de/2025/01/claude-writing-a-telegram-bot-1st-try/>Claude writing a Telegram Bot, 1st try</a> I&rsquo;m using an elaborate, step-by-step prompting strategy with Haiku model</li><li>In <a href=https://stefansiegl.de/2025/01/claude-writing-a-telegram-bot-2nd-try/>Claude writing a Telegram Bot, 2nd try</a> I aim for a detailed one-shot prompt</li><li>In <a href=https://stefansiegl.de/2025/02/claude-writing-a-telegram-bot-3rd-try/>Claude writing a Telegram Bot, 3rd try</a> I try a rather short one-shot prompt</li><li>In <a href=https://stefansiegl.de/2025/02/other-llms-writing-a-telegram-bot/>Other LLMs writing a Telegram Bot</a> I quickly try DeepSeek and some more LLMs</li></ul></div></div><p>Like in the first round, I will once more show the full dialog I&rsquo;ve used to let it generate the implementation. Then follow up with a discussion of the code and some final thoughts.</p><p>This is once more run against Claude 3.5 <strong>Haiku</strong>, so it&rsquo;s comparable to the previous two posts. This time I omitted asking for intermediate steps like, scale the image to reduce the number of tokens used, and which prompt to use.</p><blockquote class=quote-request>Create a telegram chatbot in JavaScript. I want to take a photo of a gas meter. Upload to OpenAI API to read the gas meter. The number has five digits before comma, and three decimal places. Then store the value to a postgres database table and return it in the chat.</blockquote><blockquote class=quote-response><p>Here&rsquo;s a breakdown of the implementation:</p><ol><li>Telegram Bot Setup:</li></ol><ul><li>Uses telegraf for handling Telegram bot interactions</li><li>Responds to /start command</li><li>Handles photo uploads</li></ul><ol start=2><li>Image Processing:</li></ol><ul><li>Downloads the uploaded photo</li><li>Uses OpenAI&rsquo;s GPT-4 Vision API to read the meter number</li><li>Validates the reading format (5 digits, 3 decimal places)</li></ul><ol start=3><li>Database Storage:</li></ol><ul><li>Uses PostgreSQL to store meter readings</li><li>Includes user ID, reading, and timestamp</li><li>Includes a method to set up the database table</li></ul><ol start=4><li>Error Handling:</li></ol><ul><li>Robust error handling for file downloads, API calls, and database operations</li></ul><p>Prerequisites and Setup:</p><ol><li>Install Node.js (version 16+)</li><li>Create a Telegram bot via BotFather</li><li>Get an OpenAI API key</li><li>Set up a PostgreSQL database</li><li>Install dependencies: npm install</li><li>Create a .env file with your credentials</li><li>Run the bot: npm start</li></ol><p>Notes and Recommendations:</p><ul><li>Ensure you have the latest versions of the dependencies</li><li>Implement proper error logging in production</li><li>Consider rate limiting and additional security measures</li><li>The bot currently assumes a specific meter reading format</li></ul><p>Would you like me to elaborate on any part of the implementation or explain how to set up and deploy this bot?</p></blockquote><p><a href=https://gist.github.com/stesie/3f496a10ee4961c00ebbd2a2825e71c4>This is the full code, that it spat out on the first shot</a></p><h2 id=code-review>Code Review<a hidden class=anchor aria-hidden=true href=#code-review>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-js data-lang=js><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>TelegramBot</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>require</span>(<span style=color:#e6db74>&#39;node-telegram-bot-api&#39;</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> { <span style=color:#a6e22e>Telegraf</span> } <span style=color:#f92672>=</span> <span style=color:#a6e22e>require</span>(<span style=color:#e6db74>&#39;telegraf&#39;</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> { <span style=color:#a6e22e>createClient</span> } <span style=color:#f92672>=</span> <span style=color:#a6e22e>require</span>(<span style=color:#e6db74>&#39;@supabase/supabase-js&#39;</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> { <span style=color:#a6e22e>OpenAI</span> } <span style=color:#f92672>=</span> <span style=color:#a6e22e>require</span>(<span style=color:#e6db74>&#39;openai&#39;</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>multer</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>require</span>(<span style=color:#e6db74>&#39;multer&#39;</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>axios</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>require</span>(<span style=color:#e6db74>&#39;axios&#39;</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>fs</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>require</span>(<span style=color:#e6db74>&#39;fs&#39;</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>pg</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>require</span>(<span style=color:#e6db74>&#39;pg&#39;</span>);
</span></span></code></pre></div><p>Already the initial imports are weird. So in it&rsquo;s summary comment it even mentioned, that it&rsquo;s using <code>telegraf</code> as a dependency, to implement the chat bot. However it also imported <code>node-telegram-bot-api</code> &mldr; which it doesn&rsquo;t use at all.</p><p>Likewise for the <code>@supabase/supabase-js</code> import. I mean, it&rsquo;s an interesting thought to use Supabase (which is an open source Firebase alternative, that&rsquo;s actually offering Postgres databases) &mldr; but it&rsquo;s neither what I asked for nor is it actually using it in it&rsquo;s own code later on.</p><p>Furthermore it&rsquo;s importing <code>multer</code> and even configured it in the constructor:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-js data-lang=js><span style=display:flex><span>    <span style=color:#75715e>// Multer for handling file uploads
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>upload</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>multer</span>({ 
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>dest</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;uploads/&#39;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>limits</span><span style=color:#f92672>:</span> { <span style=color:#a6e22e>fileSize</span><span style=color:#f92672>:</span> <span style=color:#ae81ff>5</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>1024</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>1024</span> } <span style=color:#75715e>// 5MB file size limit
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    });
</span></span></code></pre></div><p>&mldr; but afterward it&rsquo;s not using it !?</p><p>A nice touch, that wasn&rsquo;t present on the first two iterations is this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-js data-lang=js><span style=display:flex><span>    <span style=color:#75715e>// Start command handler
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>bot</span>.<span style=color:#a6e22e>command</span>(<span style=color:#e6db74>&#39;start&#39;</span>, (<span style=color:#a6e22e>ctx</span>) =&gt; {
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>ctx</span>.<span style=color:#a6e22e>reply</span>(<span style=color:#e6db74>&#39;Welcome! Send me a photo of your gas meter, and I\&#39;ll read and store the reading.&#39;</span>);
</span></span><span style=display:flex><span>    });
</span></span></code></pre></div><p>&mldr; I never asked for it, nor thought of it &mldr; but it&rsquo;s nice offering some &ldquo;call to action&rdquo;. Obviously it&rsquo;s not really needed, given that the bot is only a personal one and I of course know how to use it. But from a product/project point of view it makes sense.</p><p>Like the previous versions also it&rsquo;s a bit lax on assumptions, where it can put temporary files:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-js data-lang=js><span style=display:flex><span>  <span style=color:#66d9ef>async</span> <span style=color:#a6e22e>downloadFile</span>(<span style=color:#a6e22e>filePath</span>) {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>fileUrl</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>`https://api.telegram.org/file/bot</span><span style=color:#e6db74>${</span><span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>botToken</span><span style=color:#e6db74>}</span><span style=color:#e6db74>/</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>filePath</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>localFilePath</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>`uploads/</span><span style=color:#e6db74>${</span>Date.<span style=color:#a6e22e>now</span>()<span style=color:#e6db74>}</span><span style=color:#e6db74>_meter.jpg`</span>;
</span></span></code></pre></div><ul><li><p>It simply assumes, that there&rsquo;s a <code>uploads/</code> folder, and furthermore, that it&rsquo;s <em>writable</em>.</p></li><li><p>Once more it&rsquo;s not using random names for the temp file and/or check if it&rsquo;s accidentally overwriting stuff. It just writes.</p></li><li><p>&mldr; and as mentioned on the previous post, after all there&rsquo;s no reason at all, to create a temporary file in the first place. The data may just be stored into a <code>Buffer</code> directly.</p></li></ul><p>Furthermore, in case of exceptions occuring while accessing the LLM and/or the database, it&rsquo;ll fail to delete the temp file:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-js data-lang=js><span style=display:flex><span>        <span style=color:#75715e>// Reply to user
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#a6e22e>ctx</span>.<span style=color:#a6e22e>reply</span>(<span style=color:#e6db74>`Meter reading recorded: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>meterReading</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>// Clean up uploaded file
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#a6e22e>fs</span>.<span style=color:#a6e22e>unlinkSync</span>(<span style=color:#a6e22e>filePath</span>);
</span></span><span style=display:flex><span>      } <span style=color:#66d9ef>catch</span> (<span style=color:#a6e22e>error</span>) {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>error</span>(<span style=color:#e6db74>&#39;Error processing meter reading:&#39;</span>, <span style=color:#a6e22e>error</span>);
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>ctx</span>.<span style=color:#a6e22e>reply</span>(<span style=color:#e6db74>&#39;Sorry, I couldn\&#39;t process the meter reading. Please try again.&#39;</span>);
</span></span><span style=display:flex><span>      }
</span></span></code></pre></div><p>&mldr; it even thought of providing a catch block, but then simply forgot about it. Well, on the other hand, one might argue that this is intentional, given that it might aid debugging ü§∑</p><p>I like that it was able to come up with a meaningful prompt for GPT</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-js data-lang=js><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>role</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#34;user&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>content</span><span style=color:#f92672>:</span> [
</span></span><span style=display:flex><span>    { 
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>type</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#34;text&#34;</span>, 
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>text</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#34;Read the number on this gas meter. Ensure it&#39;s a 5-digit number before the comma and 3 decimal places.&#34;</span> 
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#75715e>// ...
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>&mldr; so it can mimic the standpoint of an entity of it&rsquo;s own, asking GPT as a third party.</p><p>However it doesn&rsquo;t consider the freedom in ways of answering it leaves to the GPT. Given this prompt GPT will never just return the pure number. It&rsquo;ll always at least prefix something like &ldquo;The gas meter reading is &mldr;&rdquo;.</p><p>Therefore it&rsquo;s a nice touch, that it even comes up with a regex, asserting the format:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-js data-lang=js><span style=display:flex><span>  <span style=color:#a6e22e>validateMeterReading</span>(<span style=color:#a6e22e>reading</span>) {
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Regex to match 5 digits before comma, 3 decimal places
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>meterReadingRegex</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>/^\d{5}\.\d{3}$/</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>meterReadingRegex</span>.<span style=color:#a6e22e>test</span>(<span style=color:#a6e22e>reading</span>);
</span></span><span style=display:flex><span>  }
</span></span></code></pre></div><p>&mldr; but that&rsquo;s going to fail in all the cases, given that it didn&rsquo;t tell the LLM, that it will make this assertion.</p><p>What&rsquo;s actually confusing, is that it just provided some example usage at the end of the file:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-js data-lang=js><span style=display:flex><span><span style=color:#75715e>// Export for use
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#a6e22e>module</span>.<span style=color:#a6e22e>exports</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>GasMeterBot</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Example instantiation
</span></span></span><span style=display:flex><span><span style=color:#75715e>// const gasMeterBot = new GasMeterBot();
</span></span></span><span style=display:flex><span><span style=color:#75715e>// gasMeterBot.setupDatabase();
</span></span></span></code></pre></div><p>&mldr; so it&rsquo;s actually just creating a module &mldr; that it doesn&rsquo;t instantiate anywhere. Which after all is in contrast to the &ldquo;Setup steps&rdquo; it brought forward, where it told to run <code>npm start</code>. But given that this instruction is commented out, it just will do nothing if started &mldr;</p><h2 id=comparison-with-sonnet>Comparison with Sonnet<a hidden class=anchor aria-hidden=true href=#comparison-with-sonnet>#</a></h2><p>I&rsquo;ve also quickly tried exactly the same prompt with Claude 3.5 <strong>Sonnet</strong>, which came up with the following prompt:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-js data-lang=js><span style=display:flex><span>{ 
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>type</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#34;text&#34;</span>, 
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>text</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#34;Read the gas meter value in this image. Return only the number with 5 digits before decimal and 3 after. Format: XXXXX.XXX&#34;</span> 
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>&mldr; where it clearly tells GPT which output it expects.</p><p>It also refrained from depending on other modules, which it wasn&rsquo;t using.</p><p>Neither the temporary file handling issues were present in that code, it just directly converted the <code>ArrayBuffer</code> into a <code>Buffer</code>, using <code>Buffer.from</code>.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>I think it&rsquo;s fair to say that Claude 3.5 <strong>Haiku</strong> wasn&rsquo;t able to properly cope with the given task. While it does provide a foundation to build upon, it definitely takes a human to address the issues at hand (either fixing directly, or providing further prompts to drive it forward). On the other hand it shows, that quite like you would reason about intermediate steps with (lesser experienced) human developers, you need to have this &ldquo;thinking together&rdquo; phase with the LLM as well.</p><p>The quick comparison with Claude 3.5 <strong>Sonnet</strong> shows, that that one is far more capable and could handle even this task easily.</p><div class="admonition note"><div class=admonition-icon>‚ÑπÔ∏è</div><div class=admonition-body>There&rsquo;s a follow-up to this article, where I tried the prompt from above against some other popular LLMs from DeepSeek, Meta & OpenAI: <a href=https://stefansiegl.de/2025/02/other-llms-writing-a-telegram-bot/>Other LLMs writing a Telegram Bot</a></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://stefansiegl.de/tags/ai/>AI</a></li><li><a href=https://stefansiegl.de/tags/claude/>Claude</a></li><li><a href=https://stefansiegl.de/tags/telegram/>Telegram</a></li><li><a href=https://stefansiegl.de/tags/image-recognition/>Image Recognition</a></li></ul></footer></article></main><footer class=footer><span>¬© 2025 Stefan Siegl ¬∑ all content is <a href=https://creativecommons.org/licenses/by-sa/4.0/deed>CC-BY-SA</a> ¬∑ <a href=https://www.swyx.io/digital-garden-tos#for-visitors>Terms of Service</a></span> ¬∑
<a href=/imprint>Imprint</a> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>