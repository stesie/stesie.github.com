[{"content":"For quite some time now, I\u0026rsquo;ve been using Logseq as my personal knowledge management system. It has been an invaluable tool for organizing my thoughts, projects, and ideas. A few months ago, I decided to take it a step further by incorporating journaling into my daily routine. Each morning and evening, I reflect on how I\u0026rsquo;m feeling, what has moved me, what went well during the day, where I excelled, and where there might be room for improvement.\nThis daily journaling practice has led me to spend even more time at the computer, diving into topics that interest me. Yet, I often find myself falling down rabbit holes, getting sidetracked from my main goal of reflection and wrapping up the day.\nA few weeks ago, I stumbled upon a journal entry by Brian Sunter, where he shared his thoughts on audio journaling. He uses the Voice Memos app on his iPhone to record his thoughts, then shares the file with Logseq. From there, he employs Whisper and his GPT plugin to derive the content he needs. While his idea didn\u0026rsquo;t immediately resonate with me—I couldn\u0026rsquo;t see myself journaling during a walk, nor did I want to keep audio files in my Logseq assets—it did spark an interest in exploring audio journaling further.\nThomas Frank also wrote an article on the subject and created several YouTube videos. He uses Pipedreams to push data into Notion, which is an interesting approach but more suited to Notion\u0026rsquo;s web-based API system.\nMy Current Solution To ease into audio journaling while keeping it simple, I\u0026rsquo;ve devised the following workflow:\nRecord a voice memo on my iPhone. Share the recording via the Nextcloud app. (The next morning) Process the audio file with OpenAI Whisper. Run the transcript through a language model to correct names, remove filler words, and summarize the text into bullet points. Copy and paste the results into Logseq. I\u0026rsquo;ve consolidated the last three steps into a small shell script that processes the latest file, transcribes it, and runs it through a language model. The output is automatically copied to the clipboard, allowing me to simply press Control+V in Logseq.\nAutomating Transcription OpenAI Whisper is open source and easy to install. On Arch Linux, you can set it up with sudo pacman -S python-openai-whisper. Running whisper \u0026quot;New Recording.m4a\u0026quot; --language German provides initial results. However, on my Thinkpad without a powerful graphics card, it takes quite a while.\nFor the impatient (like me), there\u0026rsquo;s a paid API option:\ncurl --request POST \\ --url https://api.openai.com/v1/audio/transcriptions \\ --header \u0026#34;Authorization: Bearer $OPENAI_API_KEY\u0026#34; \\ --header \u0026#39;Content-Type: multipart/form-data\u0026#39; \\ --form file=@\u0026#39;New Recording.m4a\u0026#39; \\ --form model=whisper-1 -F response_format=text -o out In just a second or two, you get a transcript of your lengthy recording.\nSummarizing with the Language Model Now that we have a transcript, it\u0026rsquo;s still more of a ramble than concise answers to my daily reflection questions. Software project names and people’s names might also be misspelled.\nI\u0026rsquo;ve come to appreciate the Python module llm as a CLI tool for interacting with language models. On Arch, it\u0026rsquo;s conveniently installed with python-pipx. Simply run pipx install llm. Done.\nThe advantage of this tool is its ability to pipe files through standard input and abstract different language models. Whether using a local Llama model or one hosted on AWS Bedrock, Claude, or OpenAI, the command remains the same. You can also set a template with maximum token count, temperature, and system prompt.\nHere\u0026rsquo;s the command to use:\nllm --system \u0026#34;$(cat system.txt)\u0026#34; -o max_tokens 4096 --save audio-journal To trigger it, simply run:\nllm -t audio-journal \u0026lt; out ℹ️ System Prompt\nThe system prompt is highly personal, but I\u0026rsquo;m happy to share mine for inspiration (actually mine is in German language):\nYou are a helpful assistant preparing transcribed audio notes for a journal. The focus is on evening reflection questions. The journal is in German. Summarize the answers concisely using bullet points, but form complete sentences. Remove filler words and phrases. Each bullet point should start with [[gpt]]. If a bullet point is too long, split it into two or more points.\nCorrect misspelled project and product names.\nCorrect misspelled names of people. Add last names if I only mention the first name and it\u0026rsquo;s clear. Names should also be enclosed in square brackets, e.g., [[Max Mustermann]].\nNames of friends: XXX\nNames of some colleagues: XXX\nNicknames: XXX is YYY, XXX is YYY. Replace nicknames with full names.\nNames of software projects, etc., that might be mentioned: Logseq, Todoist, Claude, Chat-GPT, Deepseek, Llama, cadiff\nYour response should be formatted as follows:\n[[Evening Questions]] #daily [[How Am I feeling?]] [[gpt]] Insert bullet points from transcript here [[gpt]] Repeat line if necessary [[What’s Something Good That Happened Today?]] [[gpt]] Insert bullet points from transcript here [[gpt]] Repeat line if necessary [[What Did I Do Well?]] [[gpt]] Insert bullet points from transcript here [[gpt]] Repeat line if necessary [[What Could I Have Done Better?]] [[gpt]] Insert bullet points from transcript here [[gpt]] Repeat line if necessary [[What Am I Thinking of?]] [[gpt]] Insert bullet points from transcript here [[gpt]] Repeat line if necessary By integrating audio journaling into my routine, I\u0026rsquo;ve found a new way to streamline my reflections and make the most of my personal knowledge management system. While it took some experimentation to find the right approach, the combination of open-source tools and creative scripting has made the process both efficient and effective.\n","permalink":"https://stefansiegl.de/2025/01/audio-journaling/","summary":"For quite some time now, I\u0026rsquo;ve been using Logseq as my personal knowledge management system. It has been an invaluable tool for organizing my thoughts, projects, and ideas. A few months ago, I decided to take it a step further by incorporating journaling into my daily routine. Each morning and evening, I reflect on how I\u0026rsquo;m feeling, what has moved me, what went well during the day, where I excelled, and where there might be room for improvement.","title":"Audio Journaling"},{"content":"This is the second part of the mini series on writing Telegram bots to read my gas meter using Claude.\nℹ️ This post belongs to a mini series (I\u0026rsquo;m aiming for three posts in a row), where I try prompting Claude with different levels of verbosity and compare the results:\nIn Claude writing a Telegram Bot, 1st try I\u0026rsquo;m using an elaborate, step-by-step prompting strategy with Haiku model In Claude writing a Telegram Bot, 2nd try I was aiming for a (still pretty detailed) one-shot prompt Like in the first round, I will first show the full dialog I\u0026rsquo;ve used to let it generate the implementation. Then following up with a discussion of the code and some final thoughts.\nThis time, I aim to provide a more concise approach by offering a single prompt that encompasses all the necessary steps to achieve our goal.\nCreate a Telegram chatbot in JavaScript. The bot receives an image that is sent to it, shall then scale it to have 1000 pixels or less on the longer edge. Then upload the image to OpenAPI gpt-4o model along with the prompt \u0026ldquo;Wie lautet der Zählerstand auf dem Foto des Gaszählers. Die Zahl hat fünf Ziffern vor dem Komma und drei Nachkommastellen. Gib nur die Zahl aus, keinen Text davor oder danach\u0026rdquo;. Take the resulting number, store it to a database table named gas_meter, along with the current timestamp. Furthermore return the extracted number within the telegram communication, so the chat contact has some feedback. I\u0026rsquo;ll help you create a Telegram chatbot in JavaScript for processing gas meter images. I\u0026rsquo;ll break this down into key components.\nThis implementation requires:\nTelegram bot token OpenAI API key PostgreSQL database connection npm packages: telegraf, axios, sharp, pg, openai Database setup:\nCREATE TABLE gas_meter ( id SERIAL PRIMARY KEY, reading NUMERIC(8,3), timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); Recommended environment variables:\nTELEGRAM_BOT_TOKEN OPENAI_API_KEY DATABASE_URL Would you like me to elaborate on any specific part of the implementation?\nLooking at the code, I immediately noticed, that I forgot to mention that I want support for .env files and also that it must expect the LLM to reply with German number formatting (using a comma to separate the decimal places), \u0026hellip; quite like I had with the first try. So let\u0026rsquo;s fix this:\nconsider that the reading, as returned by the LLM, might use a comma as decimal separator (due to the German prompt). Also read environment variables from an .env file, if it exists Key changes:\nAdded dotenv to load .env file Modified extractMeterReading() to replace comma with dot Converted reading to parseFloat() for correct number parsing Requires npm install dotenv in addition to previous dependencies.\nRunning it for the first time I noticed it\u0026rsquo;s calling the LLM wrongly\nthe code causes the following openai error:\nError processing image: BadRequestError: 400 Invalid value: \u0026lsquo;image\u0026rsquo;. Supported values are: \u0026rsquo;text\u0026rsquo;, \u0026lsquo;image_url\u0026rsquo;, \u0026lsquo;input_audio\u0026rsquo;, \u0026lsquo;refusal\u0026rsquo;, \u0026lsquo;audio\u0026rsquo;, and \u0026lsquo;file\u0026rsquo;.\nfigure out what\u0026rsquo;s wrong please\nKey changes:\nUse image_url instead of image Write image to temporary file Convert image to base64 using file-based method Ensure cleanup of temporary file This approach resolves the OpenAI API image submission issue by using the correct image_url format.\nI\u0026rsquo;ve uploaded the generated JavaScript code to GitHub.\nCode Review Compared to the results of the first iteration I like that this time it\u0026rsquo;s using axios for all http requests. Be it downloading from Telegram image storage, be it the call to the LLM.\nWhen scaling the image, I think it\u0026rsquo;s interesting how it calculates the target image size (considering the aspect ratio) manually this time, where it simply delegated that stuff to sharp with the first implementation.\nWhat I really find weird is the way how it\u0026rsquo;s converting the ArrayBuffer from sharp to a base64-encoded Buffer, which it\u0026rsquo;s passing on to the LLM. To do so it first writes the ArrayBuffer to disk, just to re-read it immediately after, like this:\nconst tempFilePath = `/tmp/gas_meter_${Date.now()}.jpg`; // Write buffer to temporary file fs.writeFileSync(tempFilePath, imageBuffer); // Read file and convert to base64 const fileBuffer = fs.readFileSync(tempFilePath); const base64Image = fileBuffer.toString(\u0026#39;base64\u0026#39;); \u0026hellip; would this be an actual code review situation, I very likely would address this. Especially since the guessable filename might pose a security risk.\nInstead this should just do the trick in one go:\nconst base64Image = Buffer.from(imageBuffer).toString(\u0026#39;base64\u0026#39;); Furthermore, the way it\u0026rsquo;s (not) handling errors from the LLM invokation, it would not remove the temporary file, if the LLM invocation fails.\nAs mentioned earlier, the above code was produced by Claude 3.5 Haiku. I\u0026rsquo;ve later on tried the same prompt with Sonnet, and that just directly converted the ArrayBuffer as shown by me.\nFinal thoughts All in all another nice solution to the problem, that just works fine :-)\nThis demonstrates that it\u0026rsquo;s not always essential to be overly wordy when giving guidance, unlike I initially did. However, breaking it down into individual steps, also aids personal understanding. Moreover, since I haven\u0026rsquo;t created a Telegram bot before and it\u0026rsquo;s been a while since I\u0026rsquo;ve scaled an image using Node.js, examining code samples in detail has helped me gain insights.\nObserving the variations in how the sharp library is used with the .resize(...) function, it brings to mind the differing approaches that two human developers might take when writing code. Similar to Claude\u0026rsquo;s example, it is possible for two individuals to reach different conclusions, with one opting to manually calculate the aspect ratio and the other choosing to utilize the library\u0026rsquo;s built-in features.\n","permalink":"https://stefansiegl.de/2025/01/claude-writing-a-telegram-bot-2nd-try/","summary":"This is the second part of the mini series on writing Telegram bots to read my gas meter using Claude.\nℹ️ This post belongs to a mini series (I\u0026rsquo;m aiming for three posts in a row), where I try prompting Claude with different levels of verbosity and compare the results:\nIn Claude writing a Telegram Bot, 1st try I\u0026rsquo;m using an elaborate, step-by-step prompting strategy with Haiku model In Claude writing a Telegram Bot, 2nd try I was aiming for a (still pretty detailed) one-shot prompt Like in the first round, I will first show the full dialog I\u0026rsquo;ve used to let it generate the implementation.","title":"Claude writing a Telegram Bot, 2nd try"},{"content":"Below you\u0026rsquo;ll find results of my initial try in using Claude to write a Telegram Bot.\nℹ️ This post belongs to a mini series (I\u0026rsquo;m aiming for three posts in a row), where I try prompting Claude with different levels of verbosity and compare the results:\nIn Claude writing a Telegram Bot, 1st try I\u0026rsquo;m using an elaborate, step-by-step prompting strategy with Haiku model In Claude writing a Telegram Bot, 2nd try I was aiming for a (still pretty detailed) one-shot prompt My ultimate goal is to figure out how much gas I need to heat my apartment. The gas meter is located in the hallway outside my apartment and doesn\u0026rsquo;t have a digital interface to tap into. Since I just rent the place and the meter is outside the apartment, I can\u0026rsquo;t tinker with it. The idea is to snap a quick picture of the meter in the morning and evening, or whenever I feel like it. Just in passing\u0026hellip; and have the value automatically stored in a database for future analysis.\nIn short, it needs to be quick and easy. My initial thought was to create an iPhone app, perhaps using Flutterflow. However, since I don\u0026rsquo;t have an Apple Developer Account, that\u0026rsquo;s a bit tricky. The next idea that came to mind was Telegram. Send the image to a bot, and let it do the rest. Since I already have an API account with OpenAI, the image will be sent there. GPT-4o will read the meter reading, store it in a PostgreSQL table, and that\u0026rsquo;s it. That\u0026rsquo;s the plan\u0026hellip;\nFirst try: being verbose while prompting step by step Before firing up Claude AI I quickly tried image recognition with GPT-4o and already played a little, what\u0026rsquo;s a proper prompt to read the meter value. Given that some numbers might be hard to read, since the meter is \u0026ldquo;in between\u0026rdquo; two values, I noticed to get more reliable results if I tell it, that the number has five digits before the comma, and three decimal places.\nAfter I found out that GPT is perfectly able to read the meter, and isn\u0026rsquo;t confused by any of the other numbers and/or text written onto the meter \u0026hellip; I wanted to find out how to write a Telegram bot. To be honest, I\u0026rsquo;ve never tried that before, \u0026hellip; neither with nor without an LLM as support.\nFurthermore I haven\u0026rsquo;t used Claude much before. But I heard that Claude would be better with JavaScript, and GPT better with Python \u0026hellip; since I\u0026rsquo;m a JavaScript/TypeScript person, I went with claude and asked for JavaScript.\nAs I\u0026rsquo;m just on the free tier, and there was much traffic on their site, Claude 3.5 Sonnet wasn\u0026rsquo;t available. Therefore I went with the Haiku model. Going step by step, I started out asking how it would write a bot in the first place\nWhich JavaScript library would you use to create a telegram bot, that I would want to send images to? For creating a Telegram bot in JavaScript that can send images, I recommend using the node-telegram-bot-api library. It provides a straightforward way to interact with the Telegram Bot API and supports sending images easily. Here\u0026rsquo;s a quick example:\nconst TelegramBot = require(\u0026#39;node-telegram-bot-api\u0026#39;); const bot = new TelegramBot(token, {polling: true}); // Sending an image bot.sendPhoto(chatId, \u0026#39;path/to/image.jpg\u0026#39;); The library handles image uploads, supports various methods for sending media, and provides a robust set of features for Telegram bot development.\nOk, so it misunderstood me, that I want to send images. No idea how that comes, but let\u0026rsquo;s correct it right away:\nNo, I wouldn\u0026rsquo;t want the bot to send images, but receive them. Does this library handle it anyways? Yes, node-telegram-bot-api handles receiving images as well. Here\u0026rsquo;s a simple example:\nbot.on(\u0026#39;photo\u0026#39;, (msg) =\u0026gt; { const photo = msg.photo; // Access the photo details and download if needed }); The library provides methods to access image metadata and download received images, making it convenient for handling image uploads to your Telegram bot.\nOk, seems like this could work. So let\u0026rsquo;s just move on to resizing the image. The camera image will initially be quite large. OpenAI doesn\u0026rsquo;t accept images bigger than 2048px anyways, and it\u0026rsquo;s billing by block sizes of 512x512 pixels, \u0026hellip; so let\u0026rsquo;s see how we can shrink the image\nok, fine. This image can be pretty big in size. How would you scale the image so it\u0026rsquo;s longer edge doesn\u0026rsquo;t have more than roughly 1000 pixels ? I\u0026rsquo;ll demonstrate how to resize an image using the Sharp library in Node.js:\nconst sharp = require(\u0026#39;sharp\u0026#39;); bot.on(\u0026#39;photo\u0026#39;, async (msg) =\u0026gt; { const fileId = msg.photo[msg.photo.length - 1].file_id; const file = await bot.getFile(fileId); const filePath = file.file_path; await sharp(filePath) .resize(1000, 1000, { fit: \u0026#39;inside\u0026#39;, withoutEnlargement: true }) .toFile(\u0026#39;resized-image.jpg\u0026#39;); }); This code:\nRetrieves the highest resolution photo file Uses Sharp to resize, maintaining aspect ratio Ensures no edge exceeds 1000 pixels Prevents image enlargement if smaller Ok, that was easy. So let\u0026rsquo;s ask it to connect to OpenAI\u0026rsquo;s GPT API. Admittedly asking Claude to do so feels a little bit mean \u0026hellip;\nNice! ok, so how would I send this image to OpenAI GPT API to run image recognition along with a prompting message \u0026ldquo;Wie lautet der Zählerstand auf dem Foto des Gaszählers. Die Zahl hat fünf Ziffern vor dem Komma und drei Nachkommastellen. Gib nur die Zahl aus, keinen Text davor oder danach\u0026rdquo; ? Here\u0026rsquo;s a code snippet to send the image to OpenAI\u0026rsquo;s API with the specified prompt:\nconst axios = require(\u0026#39;axios\u0026#39;); const fs = require(\u0026#39;fs\u0026#39;); bot.on(\u0026#39;photo\u0026#39;, async (msg) =\u0026gt; { const fileId = msg.photo[msg.photo.length - 1].file_id; const file = await bot.getFile(fileId); const filePath = file.file_path; // Resize image with Sharp await sharp(filePath) .resize(1000, 1000, { fit: \u0026#39;inside\u0026#39;, withoutEnlargement: true }) .toFile(\u0026#39;resized-image.jpg\u0026#39;); // Send to OpenAI API const response = await axios.post(\u0026#39;https://api.openai.com/v1/chat/completions\u0026#39;, { model: \u0026#34;gpt-4-vision-preview\u0026#34;, messages: [ { role: \u0026#34;user\u0026#34;, content: [ { type: \u0026#34;text\u0026#34;, text: \u0026#34;Wie lautet der Zählerstand auf dem Foto des Gaszählers. Die Zahl hat fünf Ziffern vor dem Komma und drei Nachkommastellen. Gib nur die Zahl aus, keinen Text davor oder danach\u0026#34; }, { type: \u0026#34;image_url\u0026#34;, image_url: { url: `data:image/jpeg;base64,${fs.readFileSync(\u0026#39;resized-image.jpg\u0026#39;, \u0026#39;base64\u0026#39;)}` } } ] } ], max_tokens: 300 }, { headers: { \u0026#39;Authorization\u0026#39;: `Bearer ${process.env.OPENAI_API_KEY}`, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } }); console.log(response.data.choices[0].message.content); }); Note: Requires setting OPENAI_API_KEY environment variable.\nLooks good so far. It cannot know that the model is no longer in preview. So let\u0026rsquo;s tell it that. I also wonder why it\u0026rsquo;s using axios over node-fetch, but likely it just has seen more examples with that one.\nI\u0026rsquo;m not sure why it\u0026rsquo;s using the temp file, therefore let\u0026rsquo;s briefly ask it:\nok, cool. you can just assume that meanwhile there is a model named gpt-4o-mini that\u0026rsquo;s good to use.\none question to the code, do I really need to create a temporary file on disk? (resized-image.jpg as you\u0026rsquo;ve called it)\nNo, you don\u0026rsquo;t need to create a temporary file. You can resize the image directly in memory using Sharp\u0026rsquo;s buffer methods:\nconst axios = require(\u0026#39;axios\u0026#39;); bot.on(\u0026#39;photo\u0026#39;, async (msg) =\u0026gt; { const fileId = msg.photo[msg.photo.length - 1].file_id; const file = await bot.getFile(fileId); const filePath = file.file_path; // Resize image to buffer const resizedImageBuffer = await sharp(filePath) .resize(1000, 1000, { fit: \u0026#39;inside\u0026#39;, withoutEnlargement: true }) .toBuffer(); // Send to OpenAI API const response = await axios.post(\u0026#39;https://api.openai.com/v1/chat/completions\u0026#39;, { model: \u0026#34;gpt-4o-mini\u0026#34;, messages: [ { role: \u0026#34;user\u0026#34;, content: [ { type: \u0026#34;text\u0026#34;, text: \u0026#34;Wie lautet der Zählerstand auf dem Foto des Gaszählers. Die Zahl hat fünf Ziffern vor dem Komma und drei Nachkommastellen. Gib nur die Zahl aus, keinen Text davor oder danach\u0026#34; }, { type: \u0026#34;image_url\u0026#34;, image_url: { url: `data:image/jpeg;base64,${resizedImageBuffer.toString(\u0026#39;base64\u0026#39;)}` } } ] } ], max_tokens: 300 }, { headers: { \u0026#39;Authorization\u0026#39;: `Bearer ${process.env.OPENAI_API_KEY}`, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } }); console.log(response.data.choices[0].message.content); }); This approach avoids creating a temporary file on disk.\nOkay, looks good on first sight. So let\u0026rsquo;s move on to the last step: saving things to the database:\nVery nice! Last but not least, how would I connect to a PostgreSQL database and insert a new record into a table called gas_meter, simply storing the number returned from the GPT model along with the current timestamp? Here\u0026rsquo;s how to connect to PostgreSQL and insert the gas meter reading:\nconst { Pool } = require(\u0026#39;pg\u0026#39;); const pool = new Pool({ connectionString: process.env.DATABASE_URL }); bot.on(\u0026#39;photo\u0026#39;, async (msg) =\u0026gt; { // Previous image processing and GPT API call code... const meterReading = response.data.choices[0].message.content; try { const client = await pool.connect(); await client.query( \u0026#39;INSERT INTO gas_meter (reading, timestamp) VALUES ($1, NOW())\u0026#39;, [meterReading] ); client.release(); } catch (err) { console.error(\u0026#39;Database insert error:\u0026#39;, err); } }); This code:\nUses pg library for PostgreSQL connection Connects via DATABASE_URL environment variable Inserts reading with current timestamp Handles connection and potential errors I\u0026rsquo;ve seen before, that GPT uses a comma, instead of a dot, to separate the decimal places. Let\u0026rsquo;s help Claude \u0026hellip;\nTake into account, that the GPT may answer in German locale, so using a comma as a decimal separator. The database should save the value as a floating point number Here\u0026rsquo;s the updated code to handle German number format:\nbot.on(\u0026#39;photo\u0026#39;, async (msg) =\u0026gt; { // Previous code... const meterReading = response.data.choices[0].message.content .replace(\u0026#39;,\u0026#39;, \u0026#39;.\u0026#39;); // Convert German decimal comma to dot try { const client = await pool.connect(); await client.query( \u0026#39;INSERT INTO gas_meter (reading, timestamp) VALUES ($1, NOW())\u0026#39;, [parseFloat(meterReading)] ); client.release(); } catch (err) { console.error(\u0026#39;Database insert error:\u0026#39;, err); } }); Key changes:\nReplace comma with dot Use parseFloat() to convert to a proper floating-point number Directly pass to PostgreSQL query Ok, so we have all pieces together. Let\u0026rsquo;s ask it to combine everything\nCool. So now let\u0026rsquo;s piece everything together. Create a Telegram bot in JavaScript language, that takes an images, scales it to not eat too many tokens. Feed it through GPT to read the number and store it to the database please! Requirements before running:\nSet environment variables: TELEGRAM_BOT_TOKEN OPENAI_API_KEY DATABASE_URL Install dependencies: npm install node-telegram-bot-api pg sharp axios Ensure PostgreSQL table exists with schema: CREATE TABLE gas_meter (reading FLOAT, timestamp TIMESTAMP); I like having .env files, at least while running stuff locally. So let\u0026rsquo;s ask it to add dotenv:\nModify the code so that it optionally reads from an .env file, if it exists telegram-gas-meter-bot.js, second version\nKey changes:\nAdded check for .env file existence Uses dotenv to load environment variables if file exists Requires minimal additional setup Ok, so it was easy to run now. However it didn\u0026rsquo;t work. Giving Claude a clue:\nFirst of all, connection to telegram API seems to work just fine. However it fails to process the image with the following error message\nError processing gas meter reading: Error: Input file is missing: photos/file_0.jpg\nAny idea on that one?\nAnd adding to that, what do you think of adding more log output to the code?\nThe error suggests the bot isn\u0026rsquo;t correctly downloading the Telegram file. Here\u0026rsquo;s an improved version with more logging and file download:\ntelegram-gas-meter-bot.js, third version\nKey changes:\nAdded custom file download function Extensive console logging Explicit file URL construction Added file cleanup in finally block Log at bot startup This version should:\nExplicitly download the file from Telegram Provide more detailed logging Handle file download errors more robustly Getting closer, it messed up with the temp file (in line 55 here it\u0026rsquo;s assigning a const within the try block \u0026hellip; however that\u0026rsquo;s out of scope in the finally block). Let\u0026rsquo;s tell it \u0026hellip;\nnice, almost works. the GPT call is correct and the value is even stored to the database.\nHowever it fails to delete the file in the end, error message is this one:\nReferenceError: localFilePath is not defined\n\u0026hellip; that leads me to the question, is it safe to assume the current directory is writable and we can hardcode a filename? shouldn\u0026rsquo;t this go to the temp directory and use a random filename instead?\nYou\u0026rsquo;re right. Here\u0026rsquo;s an improved version using the system\u0026rsquo;s temp directory and a random filename:\ntelegram-gas-meter-bot.js, fourth version\nKey changes:\nUse os.tmpdir() for temporary file location Generate random filename with crypto.randomBytes() Add null check for localFilePath before deletion Yay, it found out, that it needs a let before the try/catch 🙂 That one was actually working well. I just noticed that it would be nice to have some feedback\nPlease also make the bot reply with the meter reading, so as a user I have some feedback that everything worked successfully and can also verify, that the value was indeed read correctly telegram-gas-meter-bot.js, fifth version\nChanges:\nAdded bot.sendMessage() to reply with meter reading Added error message if processing fails Uses German language for messages Summary That went pretty smoothly. Including the time needed to create the Bot on Telegram and creating a fresh API key in the OpenAI backend, all together hardly took 15 minutes. After all putting together this post took me way longer 🙂\nAnd well, what more can I say, the code just works fine.\nHaving some user interface at hand now, I started out playing some more and quickly found out that GPT-4o yields way better results compared to GPT-4o-mini. The latter once failed to put the comma correctly, so it read a number 10 times too high. GPT-4o never did that.\nIf I wanted to nitpick on the code, \u0026hellip;\nI think it\u0026rsquo;s weird, that it\u0026rsquo;s using https module to initially download the image from Telegram, but then uses axios to call GPT-4o\narguably it\u0026rsquo;s unnecessary to create a temporary file \u0026hellip; especially since I asked it to remove the temp file, when it passes the scaled image to the LLM \u0026hellip; it could have thought of not using a temp file for the file download\n\u0026hellip; but hey, nothing too weird. Would it be in a code review, I likely asked to change the former and ignored the latter. And in the end, it just works, therefore I take it as it is 🙂\nℹ️ There\u0026rsquo;s a follow-up to this article, where I tried a much more concise way of prompting and compare the results: Claude writing a Telegram Bot, 2nd try. ","permalink":"https://stefansiegl.de/2025/01/claude-writing-a-telegram-bot-1st-try/","summary":"Below you\u0026rsquo;ll find results of my initial try in using Claude to write a Telegram Bot.\nℹ️ This post belongs to a mini series (I\u0026rsquo;m aiming for three posts in a row), where I try prompting Claude with different levels of verbosity and compare the results:\nIn Claude writing a Telegram Bot, 1st try I\u0026rsquo;m using an elaborate, step-by-step prompting strategy with Haiku model In Claude writing a Telegram Bot, 2nd try I was aiming for a (still pretty detailed) one-shot prompt My ultimate goal is to figure out how much gas I need to heat my apartment.","title":"Claude writing a Telegram Bot, 1st try"},{"content":"Am 29. Juni war ich auf dem wunderbaren 6. SoCraTes Day Franken (große Empfehlung, falls es eine nächste Version geben wird). In der Session „Shift Left Testing“ von Felix Tensing beleuchtete er die Herausforderungen und seine Lösungsansätze im Bereich der Software-Anforderungsabdeckung. Hierbei stellte er sein Konzept vor, das speziell das Problem von inhaltlich veralteten oder unvollständigen Jira-Tickets adressiert.\nProblemstellung Jira-Tickets oder ähnliche Systeme dienen dazu, Anforderungen und Aufgaben im Entwicklungsprozess festzuhalten. Allerdings haben sie grundsätzliche Schwächen:\nVergangenheitsbezug: Tickets spiegeln oft nur den Status zum Zeitpunkt ihrer Erstellung wider und repräsentieren nicht, was die Software aktuell tut oder tun soll.\nUnvollständigkeit: Tickets dokumentieren meist nur die Anforderungen des Product Owners (PO). Wenn Entwickler:innen aus guten Gründen von der ursprünglichen Planung abweichen, wird dies nicht zwangsläufig im Ticket nachgehalten.\nBug-Tracking: Beim Auftreten von Fehlern müssen oft zahlreiche Tickets zurückliegend überprüft werden, um die relevante Historie zu rekonstruieren.\nLösung: Fachfunktionen Felix präsentierte als Lösungsansatz das Konzept der Fachfunktionen. Diese sind langlebige, eindeutige Einheiten, die Anforderungen, ihre Umsetzung und Tests zusammenführen.\nEigenschaften der Fachfunktionen Eindeutige Kennung: Jede Fachfunktion erhält eine eindeutige ID, die nicht wiederverwendet wird.\nLebendiges Dokument: Fachfunktionen werden kontinuierlich gepflegt und aktualisiert.\nAkzeptanzkriterien (AK): Sie bestehen aus einer Reihe von Akzeptanzkriterien: Ursprünglich vom PO formuliert. Durch die Entwickler:innen detailliert und ergänzt.\nEvolution durch Stories: Neue Stories referenzieren bestehende Fachfunktionen und aktualisieren deren Akzeptanzkriterien, z. B. durch Hinzufügen oder Streichen von Kriterien.\nStruktur der Fachfunktion Eine Fachfunktion setzt sich aus zwei Teilen zusammen:\nKurz- und Langbeschreibung: Enthält eine Beschreibung der Funktionalität, dokumentiert in AsciiDoc, was die Integration von frei strukturierten Inhalten wie Tabellen ermöglicht.\nTabelle der Akzeptanzkriterien:\nEnthält eine Kurzbeschreibung sowie Links zu automatisierten Tests (hauptsächlich Unit- oder Integrationstests).\nKriterien sind spezifisch für ein Modul und vermeiden End-to-End-Abhängigkeiten. Dadurch bleiben sie auch für frühere Git-Versionen valide.\nBeispiel Technische Umsetzung AsciiDoc und YAML: Der Prosa-Teil der Fachfunktion wird in AsciiDoc verfasst, die Akzeptanzkriterien in YAML abgelegt.\nAnnotationen in Tests: Tests werden direkt mit den entsprechenden Fachfunktions-IDs annotiert.\nBuild-Prozess: Die Inhalte aus AsciiDoc und YAML werden zu einem finalen AsciiDoc-Dokument zusammengeführt und in verschiedene Formate wie HTML gerendert.\nInnovative Nutzung von Storybook Als Viewer für die gerenderten Fachfunktionen wird Storybook eingesetzt. Jede Fachfunktion wird dabei als Story oder Komponente abgebildet. Vorteile:\nTransparenz: Die Storybook-Instanz zeigt den aktuellen Stand der Fachfunktionen an, passend zur jeweiligen Umgebung.\nDeployment: Storybook wird auf den Umgebungen deployed, um den Zustand der implementierten Fachfunktionen unmittelbar sichtbar zu machen.\nZusätzliche Integration Ein Publikumsvorschlag betraf die Einbindung von Architectural Decision Records (ADRs). Diese könnten in das System integriert werden, um Architekturentscheidungen nachvollziehbar mit Fachfunktionen zu verknüpfen.\nFazit Das Konzept der Fachfunktionen bietet einen innovativen Ansatz, um die oft statische und unvollständige Dokumentation von Anforderungen zu überwinden. Durch die Kombination aus lebendiger Dokumentation, automatisierten Tests und modernen Tools wie Storybook können Entwicklungsteams effizienter und transparenter arbeiten.\n","permalink":"https://stefansiegl.de/2025/01/shift-left-testing-mit-automatischer-anforderungsabdeckung/","summary":"Am 29. Juni war ich auf dem wunderbaren 6. SoCraTes Day Franken (große Empfehlung, falls es eine nächste Version geben wird). In der Session „Shift Left Testing“ von Felix Tensing beleuchtete er die Herausforderungen und seine Lösungsansätze im Bereich der Software-Anforderungsabdeckung. Hierbei stellte er sein Konzept vor, das speziell das Problem von inhaltlich veralteten oder unvollständigen Jira-Tickets adressiert.\nProblemstellung Jira-Tickets oder ähnliche Systeme dienen dazu, Anforderungen und Aufgaben im Entwicklungsprozess festzuhalten.","title":"Shift Left Testing mit automatischer Anforderungsabdeckung"},{"content":"Lately, I\u0026rsquo;ve come across the concepts of \u0026ldquo;learning in public\u0026rdquo; and \u0026ldquo;digital gardens.\u0026rdquo; A digital garden is a unique approach to organizing and sharing knowledge in a non-linear and evolving manner. Think of it as a personal wiki where one can cultivate ideas, insights, and resources over time. Unlike traditional blogs or websites that present polished and finalized content, digital gardens focus on capturing the learning process itself, fostering exploration and serendipitous connections between different pieces of information. This approach encourages collaboration, creativity, and continuous learning in a more open and transparent way.\nA quote that deeply resonated with me is\n\u0026ldquo;Knowledge grows when shared, not hoarded.\u0026rdquo; — Unknown\nThis resonated with my approach to using Logseq as my own knowledge management system, fitting well with the concept of a digital garden. I start with an initial thought, a seed, which gradually matures. Over time, several journal entries evolve into a page (some may refer to this as a Zettelkasten, though I don\u0026rsquo;t use that term).\nI have been doing this for myself for quite some time, but now I am considering sharing some of my pages and insights, where it makes sense. For now, I don\u0026rsquo;t plan to share my entire knowledge base as it contains many intertwined personal elements.\nMore quotes :) \u0026ldquo;Learning is not a spectator sport.\u0026rdquo; — D. Blocher\n\u0026ldquo;Your unfinished thoughts might just be the spark someone else needs.\u0026rdquo; — Unknown\nAnother (false) quote I really liked is\n\u0026ldquo;If a note is not published, does it really exist?\u0026rdquo; — Erwin Schrödinger\nThe publication process The publication process is straight forward and involves these steps:\nmarking a Logseq page with the metadata public:: true, along with relevant details such as date, category, and tags\nthen using the Logseq Schrödinger plugin, which generates a ZIP archive named \u0026ldquo;publicExport.zip\u0026rdquo; after a brief processing time (the plugin itself doesn\u0026rsquo;t need any configuration, just tag at least one page as public, then it should start to do stuff \u0026ndash; and don\u0026rsquo;t be impatient, give it a second or two)\nsimply extract this archive into the /content directory of my website repository\nfollowed by a git commit and git push\na GitHub Action then takes over, using hugo to build the website and deploy it on GitHub Pages. The hugo repository is based on this Logseq Hugo Template.\nCurrently, I perform these steps manually to ensure the content is accurate, but it runs smoothly enough that I plan to automate it soon.\nFinished-ness status indication From Maggie Appleton\u0026rsquo;s wonderful post A brief History \u0026amp; Ethos of the Digital Garden I have learned, that it\u0026rsquo;s important to make clear, what the state of each and every note/post is. For this I\u0026rsquo;ve taken over her categorization scheme of\n🌱 Seedling for early ideas\n🌿 Budding for tidied up notes\n🌳 Evergreen for reasonably complete work (think of blog posts)\nTo achieve this, I\u0026rsquo;ve introduced a new page-level metadata attribute in Logseq, which I just call status. This status attribute is exported (by the Schrödinger plugin) to the front matter of the post, which is then picked up by Hugo.\nLast but not least I\u0026rsquo;ve extended the post_meta.html template to actually render that information. And also, already being there, prefixed the post\u0026rsquo;s date with a \u0026ldquo;planted:\u0026rdquo; label \u0026hellip; and also adding a \u0026ldquo;last tended\u0026rdquo; field, showing the date of last modification (if different).\n","permalink":"https://stefansiegl.de/2025/01/learning-out-loud/","summary":"Lately, I\u0026rsquo;ve come across the concepts of \u0026ldquo;learning in public\u0026rdquo; and \u0026ldquo;digital gardens.\u0026rdquo; A digital garden is a unique approach to organizing and sharing knowledge in a non-linear and evolving manner. Think of it as a personal wiki where one can cultivate ideas, insights, and resources over time. Unlike traditional blogs or websites that present polished and finalized content, digital gardens focus on capturing the learning process itself, fostering exploration and serendipitous connections between different pieces of information.","title":"Learning Out Loud"},{"content":"A few days ago, I switched to the Sway window manager after returning to GNU/Linux a few months ago. Before spending a few years in the macOS ecosystem, I was a long-time user of i3. For those unfamiliar, i3 and Sway perform the same function, but i3 is built for X.org while Sway is designed for Wayland. In this post, I’ll share my impressions, lessons learned, and the challenges I faced during this transition.\nThe Starting Point Currently, I’m using Ubuntu Linux version 24.04 (noble). Installing Sway was straightforward since it’s packaged for Ubuntu. A simple apt install sway, logging out of GNOME, and selecting Sway at the login screen were enough to get started. Having used i3 before, I wasn’t entirely lost.\nI\u0026rsquo;ve started using Arch Linux meanwhile. Installation works equally smooth there, just pacman -S sway\nGlobal Keybindings (Media Keys, Power Button, etc.) As expected, some features didn’t work out-of-the-box as they do in GNOME. However, Sway’s configurability allowed me to tailor things to my liking. For instance:\nLocking the screen with a hotkey.\nPutting the laptop to standby when the power button is pressed.\nConfiguring volume and brightness keys.\nI dug up my seven-year-old i3 configuration for inspiration and adapted it to Sway. The resulting keybindings look like this:\nbindsym --locked XF86AudioMute exec pactl set-sink-mute @DEFAULT_SINK@ toggle bindsym --locked XF86AudioRaiseVolume exec pactl set-sink-volume @DEFAULT_SINK@ +5% bindsym --locked XF86AudioLowerVolume exec pactl set-sink-volume @DEFAULT_SINK@ -5% bindsym --locked XF86AudioMicMute exec pactl set-source-mute @DEFAULT_SOURCE@ toggle bindsym --locked XF86MonBrightnessUp exec sudo brightnessctl s \u0026#39;10%+\u0026#39; bindsym --locked XF86MonBrightnessDown exec sudo brightnessctl s \u0026#39;10%-\u0026#39; bindsym --locked XF86PowerOff exec systemctl suspend bindsym $mod+l exec swaylock -f -c 200020 --clock --indicator-idle-visible swayidle: Automatic Screen Locking To ensure the screen locks automatically after five minutes of inactivity, I installed swayidle, which is conveniently available on Ubuntu. However, I quickly missed having an indication of when the lock would occur. This was particularly annoying when I didn’t want the session to lock while still sitting at my desk. The solution? swaylock-effects. This tool offers additional features, such as --fade-in and --grace parameters, providing a visual cue and a grace period before locking.\nHere’s how it’s configured in my Sway setup:\nexec swayidle -w \\ timeout 300 \u0026#39;swaylock -f -c 200020 --clock --indicator-idle-visible --fade-in 3 --grace 3\u0026#39; \\ timeout 600 \u0026#39;swaymsg \u0026#34;output * power off\u0026#34;\u0026#39; resume \u0026#39;swaymsg \u0026#34;output * power on\u0026#34;\u0026#39; \\ before-sleep \u0026#39;swaylock -f -c 200020 --clock --indicator-idle-visible\u0026#39; Preventing Screen Lock During Video Playback By default, swayidle doesn’t detect when audio or video is playing. This meant my screen would lock during YouTube videos. The solution was SwayAudioIdleInhibit, a tool that’s not available in Ubuntu’s repositories. After cloning the repository and building it, I added the following line to my configuration:\nexec sway-audio-idle-inhibit Automatic Monitor Configuration Previously, I used autorandr with i3 for automatic monitor configuration. The Wayland equivalent is kanshi, which is also packaged for Ubuntu. After installation (apt install kanshi), I created a .config/kanshi/config file:\nprofile home { output \u0026#34;Samsung Electric Company U28E850 HTPK300903\u0026#34; mode 3840x2160 position 0,0 scale 2 output eDP-1 mode 1920x1200 position 1920,0 } profile laptop-only { output eDP-1 mode 1920x1200 position 0,0 } To find which outputs are there, what their respectives names are and/or which resolutions they support, run swaymsg -t get_outputs.\nMy external monitor runs at 4K with a scale of 2, while the laptop display doesn’t use scaling. To ensure everything starts automatically and to manage workspaces between monitors, I added these lines to my Sway config:\nbindsym $mod+w focus output HDMI-A-1 bindsym $mod+v focus output eDP-1 bindsym $mod+shift+w move workspace to output HDMI-A-1 bindsym $mod+shift+v move workspace to output eDP-1 exec kanshi exec_always kill -1 $(pidof kanshi) The exec_always makes sure that upon sway reconfiguration kanshi is also re-triggered. Otherwise sway would configure the screens itself.\nDesktop Notifications Sway doesn’t include a built-in notification daemon. I chose mako, which also required building from source. Adding the following line to my configuration ensures it runs on startup:\nexec mako Screenshots I\u0026rsquo;ve decided to go with and bound it to the Print key (Fn + F9 on my ThinkPad E14). I prefer to just copy the screenshot to the clipboard, \u0026hellip; usually I\u0026rsquo;m just pasting it to the next chat window anyways.\nbindsym Print exec grimshot copy area Screenshare Turns out you need to have a look into screen sharing as well. Was a not so nice experience when I wanted to share my screen in a meeting and noticed, that Chromium no longer is able to do that.\nTo fix this\nmake sure to install xdg-desktop-portal-wlr and\nadd the following to sway config exec dbus-update-activation-environment --systemd WAYLAND_DISPLAY XDG_CURRENT_DESKTOP=sway\n","permalink":"https://stefansiegl.de/2025/01/switching-to-sway/","summary":"A few days ago, I switched to the Sway window manager after returning to GNU/Linux a few months ago. Before spending a few years in the macOS ecosystem, I was a long-time user of i3. For those unfamiliar, i3 and Sway perform the same function, but i3 is built for X.org while Sway is designed for Wayland. In this post, I’ll share my impressions, lessons learned, and the challenges I faced during this transition.","title":"Switching to Sway - My Journey Back to Linux"},{"content":"About 15 years ago, I stumbled upon the Dvorak keyboard layout, an alternative to the ubiquitous QWERTY layout designed for ergonomic and efficient typing. During my exploration, I discovered Roland Kaufmann’s blog post on the Programmer Dvorak Keyboard Layout, which resonated deeply with me. Kaufmann thoughtfully considered the symbols developers frequently use and how they align with the Dvorak philosophy of optimizing for typing efficiency.\nBack then, I was using the standard US layout, employing creative methods to access German umlauts—either through AltGr combinations (e.g., AltGr + A for \u0026ldquo;ä\u0026rdquo;) or dead keys (\u0026ldquo;ae\u0026rdquo; for \u0026ldquo;ä\u0026rdquo;). However, Kaufmann\u0026rsquo;s insights inspired me to retrain my brain and fingers for Dvorak.\nInitially, the transition was frustrating. On QWERTY, I was a decently fast typist, but switching to Dvorak felt like starting from scratch. Over time, my fingers adapted, and Dvorak became my everyday keyboard layout for several years.\nIn essence, this is what the layout looked like (numbers in natural order, yet still with Shift modifier): A Pause in the Journey This phase ended abruptly about six years ago when I joined a client project that required me to use a locked-down Windows laptop. While I could configure the keyboard layout, Programmer Dvorak wasn’t an option—custom layouts were off the table. Reluctantly, I returned to the standard German (DE) layout. The switch felt like letting go of a quirky yet cherished habit, but practicality won.\nA Christmas Experiment Fast forward to this Christmas break. I found myself wondering: after all these years, could my muscle memory still recall Programmer Dvorak? To my surprise, the answer was a resounding yes!\nAs I sat down to test it, my fingers remembered the layout far better than my conscious mind did. When I overthought it—trying to mentally locate keys—I was prone to mistakes. But when I let my fingers take over, they instinctively found their way to the right keys.\nCustomizing My Setup Alongside this experiment, I started using Sway, a Wayland compositor, and wanted to configure it to support my old layout. After some research, I created an XKB configuration file to define my customized Programmer Dvorak with German umlauts. Here\u0026rsquo;s what I did:\nStep 1: Create the XKB Config File I placed the following in ~/.xkb/symbols/us-dvp-german-umlaut:\ndefault partial alphanumeric_keys xkb_symbols \u0026#34;basic\u0026#34; { include \u0026#34;us(dvp)\u0026#34; include \u0026#34;level3(ralt_switch)\u0026#34; name[Group1] = \u0026#34;English (US, Dvorak with German umlaut)\u0026#34;; key \u0026lt;AE01\u0026gt; { [ ampersand, 1, ampersand, 1 ] }; key \u0026lt;AE02\u0026gt; { [ bracketleft, 2, bracketleft, 2, currency ] }; key \u0026lt;AE03\u0026gt; { [ braceleft, 3, braceleft, 3, cent ] }; key \u0026lt;AE04\u0026gt; { [ braceright, 4, braceright, 4, yen ] }; key \u0026lt;AE05\u0026gt; { [ parenleft, 5, parenleft, 5, EuroSign ] }; key \u0026lt;AE06\u0026gt; { [ equal, 6, equal, 6, sterling ] }; key \u0026lt;AE07\u0026gt; { [ asterisk, 7, asterisk, 7 ] }; key \u0026lt;AE08\u0026gt; { [ parenright, 8, parenright, 8, onehalf ] }; key \u0026lt;AE09\u0026gt; { [ plus, 9, plus, 9 ] }; key \u0026lt;AE10\u0026gt; { [ bracketright, 0, bracketright, 0 ] }; key \u0026lt;AE11\u0026gt; { [ exclam, percent, exclam, percent, exclamdown ] }; key \u0026lt;AE12\u0026gt; { [ numbersign, grave, numbersign, grave ] }; key \u0026lt;AC01\u0026gt; { [ a, A, adiaeresis, Adiaeresis ] }; key \u0026lt;AC02\u0026gt; { [ o, O, odiaeresis, Odiaeresis ] }; key \u0026lt;AC03\u0026gt; { [ e, E, EuroSign, cent ] }; key \u0026lt;AC04\u0026gt; { [ u, U, udiaeresis, Udiaeresis ] }; }; Step 2: Update the Sway Config In the Sway configuration file, I activated the custom layout for all keyboards:\ninput \u0026#34;type:keyboard\u0026#34; { xkb_layout us-dvp-german-umlaut } This setup builds upon the default Programmer Dvorak (us(dvp)) layout, reconfigures the top row for natural numeric order, and maps German umlauts to AltGr combinations.\nThe Aftermath While reconnecting with Programmer Dvorak definitely was a nice experiment, \u0026hellip; I switched back to regular German keyboard layout after the Christmas break. While regular typing went decently well, I struggled and didn\u0026rsquo;t feel well with the keyboard shortcuts (for example in IntelliJ), many of them get awkward easily.\n","permalink":"https://stefansiegl.de/2024/12/rediscovering-programmer-dvorak/","summary":"About 15 years ago, I stumbled upon the Dvorak keyboard layout, an alternative to the ubiquitous QWERTY layout designed for ergonomic and efficient typing. During my exploration, I discovered Roland Kaufmann’s blog post on the Programmer Dvorak Keyboard Layout, which resonated deeply with me. Kaufmann thoughtfully considered the symbols developers frequently use and how they align with the Dvorak philosophy of optimizing for typing efficiency.\nBack then, I was using the standard US layout, employing creative methods to access German umlauts—either through AltGr combinations (e.","title":"Rediscovering Programmer Dvorak"},{"content":"Today (err, recently) I learned \u0026hellip; that JUnit\u0026rsquo;s extension API allows for parameter resolvers. These kick in every time you use arguments on a test method, lifecycle method or class constructor. For me, so far, none of these methods ever took an argument. But turns out, it\u0026rsquo;s possible \u0026hellip; and even useful.\nThey have a primitive example here where they allow a test method to take a random number like so:\n@Test void shouldDoStuff(@Random int number) { // actually do stuff here ... } \u0026hellip; in essence you just need an annotation interface, that\u0026rsquo;s linked to an extension class, that implements ParameterResolver interface (which just has two methods to implement).\nMy Use-case Recently I\u0026rsquo;ve been spending my spare time on a tool called cadiff, which is about diff-ing bpmn diagram files. Literally all of the integration tests need to load two bpmn files, parse them and then actually start testing/asserting stuff. This easily starts to bloat your tests.\nEnter Parameter Resolvers, my tests now look like this (at least the setup code):\n@Nested public class WithRef extends AbstractComparePatchIT { public WithRef(@BpmnFile(\u0026#34;empty-diagram.bpmn\u0026#34;) final BpmnModelInstance from, @BpmnFile(\u0026#34;error-end-event-with-ref.bpmn\u0026#34;) final BpmnModelInstance to) { super(from, to); } // actual tests ... } Resources Implementation of @BpmnFile and parameter resolver\nErrorEndEventIT class as an example\n","permalink":"https://stefansiegl.de/2024/12/junit-parameter-resolvers/","summary":"Today (err, recently) I learned \u0026hellip; that JUnit\u0026rsquo;s extension API allows for parameter resolvers. These kick in every time you use arguments on a test method, lifecycle method or class constructor. For me, so far, none of these methods ever took an argument. But turns out, it\u0026rsquo;s possible \u0026hellip; and even useful.\nThey have a primitive example here where they allow a test method to take a random number like so:","title":"JUnit Parameter Resolvers"},{"content":"Today, I discovered a powerful feature in JUnit: the Assumptions API. This API allows you to define assumptions for your tests. If an assumption isn\u0026rsquo;t met, the test execution is aborted rather than marked as failed. This distinction is crucial in scenarios like conditional test execution in CI pipelines. When a test is skipped due to an unmet assumption, it appears as \u0026ldquo;skipped\u0026rdquo; in the test reports, not as \u0026ldquo;passed\u0026rdquo; or \u0026ldquo;failed.\u0026rdquo;\nHow It Works Under the hood, the Assumptions API uses a TestAbortedException to terminate the test execution gracefully. Here\u0026rsquo;s a simple example:\nassumeTrue(\u0026#34;CI\u0026#34;.equals(System.getenv(\u0026#34;ENV\u0026#34;))); If the condition in assumeTrue is false, the test will halt, signaling that the environment wasn\u0026rsquo;t suitable for execution.\nThis is far more elegant and meaningful than manually logging and exiting the test with constructs like:\nif (!condition) { log.info(\u0026#34;cannot run\u0026#34;); assertTrue(true); return; } Avoid using such patterns—they clutter the code and lack the semantic clarity offered by the Assumptions API.\nRelevant JUnit documentation: https://junit.org/junit5/docs/current/user-guide/#writing-tests-assumptions\n","permalink":"https://stefansiegl.de/2024/08/junit-assumptions-api/","summary":"Today, I discovered a powerful feature in JUnit: the Assumptions API. This API allows you to define assumptions for your tests. If an assumption isn\u0026rsquo;t met, the test execution is aborted rather than marked as failed. This distinction is crucial in scenarios like conditional test execution in CI pipelines. When a test is skipped due to an unmet assumption, it appears as \u0026ldquo;skipped\u0026rdquo; in the test reports, not as \u0026ldquo;passed\u0026rdquo; or \u0026ldquo;failed.","title":"JUnit Assumptions API"},{"content":"Ein Erfahrungsbericht, weil ich …\nmal wieder was mit Geodaten machen wollte (und mich OpenStreetMap schon eine ganze Weile immer wieder umtreibt)\nschon länger darüber nachdenke, wie gut es wohl funktioniert, einen zufälligen Fahrrad-Routenvorschlag nach eigenem Gusto zu erzeugen\nschon länger im Fediverse (Mastodon) bin und während des Alle-Verlassen-Twitter-Hypes einen Bot zu bauen nahe liegt\nAlso los geht’s. How hard can it be?\nZunächst soll nicht unerwähnt bleiben, dass Dienste wie Strava erlauben, eine zufällige Route zu erzeugen; meinen Geschmack treffen die Ergebnisse jedoch eher selten. Wenn man “nur Straße” auswählt, wird man häufig über Staats- und teilweise auch Bundesstraßen geführt, was ich (insbesondere alleine) ätzend finde.\nKartenmaterial Ganz grundsätzlich brauchen wir zunächst einmal Kartenmaterial, weil eine Route ohne Straßen und Karte nicht funktioniert. Zum Glück gibt es das großartige Projekt OpenStreetMap, bei dem viele Freiwillige Kartendaten zusammen tragen. Meist in sehr hoher Qualität, und das auch was Nebenstraßen angeht. Und gerade dort macht es mit dem Rad auch einen großen Unterschied, wie es um Belag und Oberflächenbeschaffenheit bestellt ist.\nOpenStreetMap ist erstmal eine große Datenbank, in der alle möglichen Informationen “gemapped” sind. Das geht beim Offensichtlichen los: Straßen und Häuser. Umfasst aber auch Gleise, Läden \u0026amp; Cafes, Stromtrassen, Bushaltestellen, Glascontainer, Mülleimer, Bänke … kurzum alles, was irgendwie mit Geodaten zu tun hat und zu dem sich bisher Freiwillige fanden, es einzupflegen. An diesen Elementen hängen dann mit Schlüssel-Wert-Paaren weitere Informationen, wie beispielsweise Straßenbelag, Wegbeschaffenheit, Gehsteig vorhanden, Beleuchtet, etc.\nUns interessieren davon erstmal nur Wege. Und da nur die Typen highway (Straße), track (sonstige Wege) und path (Pfade). Die Straßen sind dabei weiter detailliert, sodass man Bundesstraßen (primary), Staatsstraßen (secondary), Kreisstraßen (tertiary), Ortsverbindungsstraßen (unclassified), etc. auseinanderhalten kann.\nEin (naiver) erster Ansatz Wir haben einen Startpunkt und eine grobe Idee von der Streckenlänge. Anschließend …\nberechnen wir den Radius eines Kreises, der im Umfang der gewünschten Streckenlänge entspricht\nwählen wir einen zufälligen Kreis dieses Radiuses, der unseren Startpunkt schneidet (damit bestimmen wir die Himmelsrichtung in die die Runde geht)\nwählen wir fünf zufällige Checkpoints, die auf diesem Kreis liegen (wobei einer der Startpunkt ist)\nsuchen wir für jeden Checkpoint einen Punkt, den wir potentiell anfahren wollen/können (anders ausgedrückt: suche den nächst gelegenen Punkt, der auf einer Kreis- oder Ortsverbindungsstraße liegt – schließlich wollen wir nicht viermal auf den Acker laufen)\nsuchen wir eine Route von Checkpoint zu Checkpoint\n… damit haben wir eine Route. Um diese dann auf Mastodon zu bringen, brauchen wir einerseits einen API-Client, andererseits noch eine Möglichkeit, die Route auf einer Karte darzustellen. Weil ohne Bild wäre der Tröt (“Tweet” in Mastodon-Lingo) wenig attraktiv.\nBestandsaufnahme Nachdem wir das nicht alles selbst bauen wollen, brauchen wir im Wesentlichen also\neinen “fernsteuerbaren” Router\neine Möglichkeit, wie wir die nächstgelegene geeignete Straße zu einem gegebenen Punkt finden können.\nBesagter Router muss nicht nur skriptbar sein und auf OpenStreetMap fußen, sondern soll flexibel/clever genug sein was die Streckenauswahl betrifft und – optional – auch das Höhenprofil berücksichtigen.\nHier kommt BRouter ins Spiel. Das ist eine Java-Anwendung (ursprünglich für Android), die man mit einem Routing-Profil füttern kann, und dann via HTTP die “beste” Strecke zwischen zwei (oder mehr) Punkten abfragen kann. Besagter BRouter bringt auch gleich ein Profil mit dem tollen Namen fastbike-verylowtraffic mit, das genau das tut was es verspricht: nur befestigte Wege, wenn möglich Kreis- oder Ortsverbindungsstraßen und Radwege. Standardmäßig ohne Berücksichtigung der Höhenmeter, das kann man auf Wunsch jedoch aktivieren.\nBei der initialen Suche nach geeigneten Checkpoints bietet sich die Overpass API an. Dieser kann man beliebige Anfragen in einer speziellen Abfragesprache (Overpass Query Language) stellen. Im einfachsten Fall die Suche nach z. B. einem Geldautomaten in einem bestimmten Bereich (typischerweise rechteckig). Es gibt jedoch auch einen around-Filter, mit dem man Objekte relativ zu anderen Elementen suchen kann.\nDie Abfrage selbst wirkt zunächst arkan:\n[out:json]; (way [highway~\u0026#39;tertiary|unclassified\u0026#39;] (around:1000.0,49.84178672462459,10.06466533810437); \u0026gt;; ); out; Sehen wir uns die Abfrage genauer an:\nwir fordern eine Antwort in JSON (statt XML)\nsuchen einen/alle Weg/e\nmit Attribut highway=tertiary oder highway=unclassified\nim Umkreis von 1000 Metern um einen gegebenen Punkt\nvon den so selektierten Wegen sollen sodann die Punkte (aka parents) abgefragt werden (\u0026gt;;)\nund das Ergebnis wollen wir auch haben (out;)\nUmsetzung Meine Lieblingsprogrammiersprache ist TypeScript. Eine kurze Recherche hat gezeigt, dass es gut gepflegte NPM-Module zum Ansprechen von Overpass (overpass-ts) und für erdräumliche Berechnungen (@turf/turf) gibt.\nAlle Funktionen von turf basieren auf GeoJSON, einem offenem Format, um eben geografische Daten in Form sog. Features zu repräsentieren. Solche Features können dann Punkte, Linien oder Polygone sein.\nUm den Ablauf des Skriptes transparenter zu bekommen, bietet es sich an, sämtliche Zwischenschritte als GeoJSON-FeatureCollection abzuspeichern. Unter geojson.io findet sich ein Online-Viewer, in den man jenes GeoJSON dann kopieren (und sodann betrachten) kann.\nAn diesem lässt sich dann gut erkennen, wie die Anwendung Schritt für Schritt vorgeht:\nWas passiert hier eigentlich? Sehen wir uns nun einmal im Detail an, was auf diesem Bild alles zu sehen ist:\nder Startpunkt (= grauer Marker) direkt in Würzburg\nKreis um diesen ziehen und einen Punkt davon als Routenmittelpunkt bestimmen (= grüner Marker)\nKreis um den grünen Marker bestimmen und fünf Checkpunkte auf diesem festlegen (= graues Polygon)\nfür alle Ecken (mit Overpass) die nächstgelegene (geeignete) Straße finden (= gelbliche Marker/Polygon)\nmit BRouter Routen zwischen ebendiesen Punkten erstellen (= rote Linien)\n“dead ends” entfernen, also Wege, die zu gelben Markern führen und dann direkt wieder zurück (= rote Marker)\nDie Koordinaten von Startpunkt und (korrigierten) Checkpoint-Markern können wir dann ein letztes mal dem BRouter füttern, der daraus eine GPX-Datei zaubert. GPX (= GPS Exchange Format) ist ein offenes XML-Format, mit dem Routen beschrieben werden können und das von den üblichen GPS-Geräten verstanden wird.\nDer o.g. Viewer für GeoJSON kann übrigens, trotz des Namens, auch GPX laden.\nGrafik rendern Ein Tröt ohne Bild ist langweilig. Aber wie bekommen wir einen Screenshot einer Karte mit unserer Route darauf?\nVielleicht nicht die effizienteste Lösung, aber zumindest straight forward: wir nehmen einfach einen Browser plus die Standard-JavaScript-Library zum Kartenrendering: Leaflet nebst GPX-Track-Plugin.\nNun brauchen wir nur noch etwas, das uns den Screenshot auf Kommando anfertigt: Googles Rendertron. Rendertron kann man auch wieder einfach per HTTP aufrufen, die URL übergeben (+ gewünschte Größe) und bekommt den Screenshot im JPG-Format zurück.\nAutomatisch Tröten Jetzt brauchen wir noch einen (Bot-)Account auf der Mastodon-Instanz unserer Wahl. In meinem Fall ist das wue.social. Und ein CLI-Tool, mit dem wir Route nebst Bild tröten können: toot.\nDas Ergebnis all der Arbeit? Ein Tröt:\nDamit kann unser Mashup auf die Zielgerade: noch ein bisschen Docker und ein paar Cronjobs – fertig ist der Bot :-)\n… und der ganze Rest Natürlich kann ich euch viel erzählen – ihr könnt euch aber auch selbst vom Bot überzeugen:\nProfilseite des Bots: @rbrb\nQuellcode des Projekts\n","permalink":"https://stefansiegl.de/2022/05/rb-road-bike-route-bot-auf-mastodon/","summary":"Ein Erfahrungsbericht, weil ich …\nmal wieder was mit Geodaten machen wollte (und mich OpenStreetMap schon eine ganze Weile immer wieder umtreibt)\nschon länger darüber nachdenke, wie gut es wohl funktioniert, einen zufälligen Fahrrad-Routenvorschlag nach eigenem Gusto zu erzeugen\nschon länger im Fediverse (Mastodon) bin und während des Alle-Verlassen-Twitter-Hypes einen Bot zu bauen nahe liegt\nAlso los geht’s. How hard can it be?\nZunächst soll nicht unerwähnt bleiben, dass Dienste wie Strava erlauben, eine zufällige Route zu erzeugen; meinen Geschmack treffen die Ergebnisse jedoch eher selten.","title":"(rb)² – Road Bike Route Bot auf Mastodon"},{"content":"Lucky me, living in the metropolitan area of Nuremberg, \u0026hellip; this year I attended three different developer-oriented open spaces hosted within one hour driving range from home.\ndevops camp in Nuremberg, six iterations so far, first one back in March 2014 And starting this year there are now two developer-centric events (both more or less claiming to be the first one):\ndevelopercamp in Würzburg, back in September 2016\nswecamp in Tennenlohe/Erlangen, right yesterday \u0026amp; today\nAt the closing event of swecamp one participant mentioned that he was from Würzburg and also attended the developercamp\u0026hellip; and that swecamp was really cool yet not on par with developercamp. Unfortunately he wasn\u0026rsquo;t willing to elaborate on what caused his feeling, but it kindof struck me \u0026hellip; I couldn\u0026rsquo;t really answer the question which of those two camps was better (and remained quiet during the retrospective)\u0026hellip;\nBut it kept bothering me and I had some time to think on may way home, \u0026hellip; however I still cannot tell which one was \u0026ldquo;better\u0026rdquo;, mainly since it doesn\u0026rsquo;t feel right to compare the incomparible. After all both events attracted (slightly) different kinds of people. And as barcamps are largely shaped by their participators it\u0026rsquo;s due to this fact that both events felt different\u0026hellip;\ndevelopercamp was organized by Mayflower which is a project \u0026amp; consulting agency in PHP \u0026amp; JavaScript field, also doing some Agile consulting, etc. \u0026hellip; they mainly attracted (and this is personal gut feeling, not statistics) Webbish people, mostly developers but also a lot of product owners. Most attendees had an e-commerce background, be it a webshop, some shop software itself (Benjamin from Shopware was there) or some middleware provider. Topics were manifold, from What\u0026rsquo;s the job of a software architect? (which was interesting with half the audience from an Agile background and the other half from a more waterfallish one) over various Product Owner and discovery focussed stuff to dev-centric like Christopher\u0026rsquo;s TypeScript Game Engine Primer. Yet most sessions felt like fitting into my personal PHP/JavaScript/CleanCode filter bubble.\nOpposed to that swecamp was organized by methodpark which also does projects and consulting, yet in more enterprisy contexts: medical devices and automotive. They managed to attract people from a (perceived) broader range of backgrounds \u0026hellip; of course there were many of their own employees (Java, Embedded), more Java and C#, C++ and embedded devs from companies nearby \u0026hellip; and of course also some Webbies (like me). The session topics were a bit more focussed on the engineering part and theoretical (read: language agnostic and less hands-on-code oriented) and a bit more testing. There were no Product Owner things and interestingly no operations stuff (Puppet, Ansible et al) and IoT was more of a topic (unfortunately I completely missed the IoT hack space)\nWhat was especially cool with swecamp was that they had Susume Sasabe from Japan as a special guest, who did a comparison of DEV culture in Japan and Germany. He also spoke about Kaizen, different approaches to knowledge transfer, different problem solving methods etc. \u0026hellip; all in all enriching the whole event every here and there. Besides that I really enjoyed @NullPlusEins\u0026rsquo;s sessions on (developer) psychology.\nI also happily noticed that CQRS and Event Sourcing were a topic on both events, maybe a little bit more focussed on DDD at swecamp. This isn\u0026rsquo;t too surprising because of all the microservices buzz (which also was a hot topic on both events). Again serverless was not a topic, meh.\nLast but not least swecamp was hosted in methodpark\u0026rsquo;s office which was way cosier and more comfortable, more decorated than the university building in Würzburg (where developercamp took place).\nTo sum it up, both events have their unique points. I really enjoyed developercamp (as a Web developer) and also swecamp, both had some sessions that really resonated with me (and after all that\u0026rsquo;s the main reason for me to go to unconferences: learning about things I don\u0026rsquo;t know that I don\u0026rsquo;t know them). I\u0026rsquo;m so happy that I don\u0026rsquo;t have to pick a single event to go to next year. Both organizers already told that they\u0026rsquo;ll follow up with a second iteration (and I\u0026rsquo;m more then willing to go to both of them)\nPS: as a funny note, the devops camp site is the only one hosted with plain, unencrypted HTTP. Both more developer focussed events feature SSL :-)\nPPS: developercamp also hosts on IPv6 and their hosting also supports Forward Secrecy, so let\u0026rsquo;s consider it better. SCNR :-)\n","permalink":"https://stefansiegl.de/2016/11/comparing-the-incomparible/","summary":"Lucky me, living in the metropolitan area of Nuremberg, \u0026hellip; this year I attended three different developer-oriented open spaces hosted within one hour driving range from home.\ndevops camp in Nuremberg, six iterations so far, first one back in March 2014 And starting this year there are now two developer-centric events (both more or less claiming to be the first one):\ndevelopercamp in Würzburg, back in September 2016\nswecamp in Tennenlohe/Erlangen, right yesterday \u0026amp; today","title":"comparing the incomparible -"},{"content":"As Ansbach (and the region around it) neither has a vibrant developer community nor a (regular) meetup to attract people to share their knowledge, mainly @niklas_heer and me felt like having to get active\u0026hellip;\nTherefore we came up with the idea to host a monthly meetup named /dev/night at @Tradebyte office (from August on regularly every 2nd Tuesday evening), give a short talk to provide food for thought and afterwards tackle a small challenge together.\n\u0026hellip; looking for some initial topics we noticed that patterns are definitely useful to stay on track and that there are many good ones beyond the good old GoF patterns. And as both of us are working for an eCommerce middleware provider we came to eCommerce patterns \u0026hellip; and finally decided to go with Transactional Patterns for the first meeting.\nSo yesterday @niklas_heer gave a small presentation on what ACID really means and why it is useful beyond database management system design (ever thought of implementing an automated teller machine? or maybe to stick with eCommerce what about fetching DHL labels from a web-service if you\u0026rsquo;re immediately charged for them? You definitely want to make sure that you don\u0026rsquo;t fetch them twice if two requests hit your system simultaneously). Besides he showed how to use two-phase commit to construct composite transactions from multiple, smaller ACID-compliant transactions and how this can aid (i.e. simplify) your system\u0026rsquo;s architecture.\nAs a challenge we thought of implementing a fictitious, distributed Club Mate vending machine, \u0026hellip; where you\u0026rsquo;ve got one central \u0026ldquo;controller\u0026rdquo; service that drives another (remote) service doing the cash handling (money collection and provide change as needed) as well as a Club Mate dispensing service (that of course also tracks its stock). Obviously it is the controller\u0026rsquo;s task to make sure that no Mate is dispensed if money collection fails, nor should the customer be charged if there\u0026rsquo;s not enough stock left.\n\u0026hellip; this story feels a bit constructed, but it fits the two-phase commit idea well and also suits the microservice bandwagon :-)\nLearnings the challenge we came up with was (again) too large \u0026ndash; quite like last Thursday when I was hosting the Pig Latin Kata in Nuremberg \u0026hellip; the team forming and getting the infrastructure working took way longer than expected (one team couldn\u0026rsquo;t even start to implement the transaction handling, as they got lost to details earlier on)\nafter all implementing a distributed system was funny, even so we couldn\u0026rsquo;t do a final test drive together (as not all of the services were feature complete)\n\u0026hellip; and it\u0026rsquo;s a refreshing difference to \u0026ldquo;just doing yet another kata\u0026rdquo;\nthe chosen topic Transactional Patterns turned out to be a good one, @sd_alt told us that he recently implemented some logic which would have benefitted from this pattern\none participant was new to test-driven development (hence his primary takeaway was how to do that with PHP and phpspec/codeception)\nthis also emphasises that we should address developers not familiar with TDD in our invitation (and should try not to scare them away by asking to bring laptops with an installed TDD-ready environment with them)\nfor visitors from Nuremberg 6:30 was too early, they ask to start at 7 o\u0026rsquo;clock\nall participants want us to carry on :-)\n\u0026hellip; so the next /dev/night/ is about to take place on September 13, 2016 at 7:10 p.m. The topic is going to be Command Query Responsibility Segregation pattern and Event Sourcing.\n","permalink":"https://stefansiegl.de/2016/08/first-dev-night/","summary":"As Ansbach (and the region around it) neither has a vibrant developer community nor a (regular) meetup to attract people to share their knowledge, mainly @niklas_heer and me felt like having to get active\u0026hellip;\nTherefore we came up with the idea to host a monthly meetup named /dev/night at @Tradebyte office (from August on regularly every 2nd Tuesday evening), give a short talk to provide food for thought and afterwards tackle a small challenge together.","title":"Starting a local developer meetup"},{"content":"Yesterday I ran the Pig Latin kata at the local software craftsmenship meetup in Nuremberg. Picking Pig Latin as the kata to do was more a coincidence than planned, but it turned out to be an interesting choice.\nSo what I\u0026rsquo;ve prepared were four user stories (from which we only tackeled two; one team did three), going like this:\n(if you\u0026rsquo;d like to do the kata refrain from reading ahead and do one story after another)\nPig Latin is an English language game that alters each word of a phrase/sentence, individually.\nStory 1:\na phrase is made up of several words, all lowercase, split by a single space\nif the word starts with a vowel, the transformed word is simply the input + \u0026ldquo;ay\u0026rdquo; (e.g. \u0026ldquo;apple\u0026rdquo; -\u0026gt; \u0026ldquo;appleay\u0026rdquo;)\nin case the word begins with a consonant, the consonant is first moved to the end, then the \u0026ldquo;ay\u0026rdquo; is appended likewise (e.g. \u0026ldquo;bird\u0026rdquo; -\u0026gt; \u0026ldquo;irdbay\u0026rdquo;)\ntest case for a whole phrase (\u0026ldquo;a yellow bird\u0026rdquo; -\u0026gt; \u0026ldquo;aay ellowyay irdbay\u0026rdquo;)\nStory 2:\nhandle consonant clusters \u0026ldquo;ch\u0026rdquo;, \u0026ldquo;qu\u0026rdquo;, \u0026ldquo;th\u0026rdquo;, \u0026ldquo;thr\u0026rdquo;, \u0026ldquo;sch\u0026rdquo; and any consonant + \u0026ldquo;qu\u0026rdquo; at the word\u0026rsquo;s beginning like a single consonant (e.g. \u0026ldquo;chair\u0026rdquo; -\u0026gt; \u0026ldquo;airchay\u0026rdquo;, \u0026ldquo;square\u0026rdquo; -\u0026gt; \u0026ldquo;aresquay\u0026rdquo;, \u0026ldquo;thread\u0026rdquo; -\u0026gt; \u0026ldquo;eadthray\u0026rdquo;)\nhandle \u0026ldquo;xr\u0026rdquo; and \u0026ldquo;yt\u0026rdquo; at the word\u0026rsquo;s beginning like vowels (\u0026ldquo;xray\u0026rdquo; -\u0026gt; \u0026ldquo;xrayay\u0026rdquo;)\nStory 3:\nuppercase input should yield uppercase output (i.e. \u0026ldquo;APPLE\u0026rdquo; -\u0026gt; \u0026ldquo;APPLEAY\u0026rdquo;)\nalso titlecase input should be kept intact, the first letter should still be uppercase (i.e. \u0026ldquo;Bird\u0026rdquo; -\u0026gt; \u0026ldquo;Irdbay\u0026rdquo;)\nStory 4:\nhandle commas, dashes, fullstops, etc. well The End. Don\u0026rsquo;t read on if you\u0026rsquo;d like to do the kata yourself.\nFindings When I was running this kata at Softwerkskammer meetup we had eight participants, who interestingly formed three groups (mostly with three people each), instead of like four pairs. The chosen languages were Java, Java Script (ES6) and (thanks to Gabor) Haskell :-)\n\u0026hellip; the Haskell group unfortunately didn\u0026rsquo;t do test first development, but I think even if they would have they\u0026rsquo;d anyways have been the fastest team. Since the whole kata is about data transformation the functional aspects really pay off here. What I really found interesting regarding their implementation of story 3 was that they kept their transformation function for lowercase words unmodified (like I would have expected) but before that detected the word\u0026rsquo;s case and build a pair consisting of the lower case word plus a transformation function to restore the casing afterwards. When I did the kata on my own I kept the case in a variable and then used some conditionals (which I think is a bit less elegant) \u0026hellip;\nBesides that feedback was positive and we had a lot of fun doing the kata.\n\u0026hellip; and as a facilitator I underestimated how long it takes the pairs/teams to form, choose a test framework and get started. Actually I did the kata myself with a stopwatch, measuring how long each step would take as I was nervous that my four stories wouldn\u0026rsquo;t be enough :-) \u0026hellip; turns out we spent more time exercising and didn\u0026rsquo;t even finish all stories.\nFurther material: Pig Latin on exercism.io\nmy solution in Java 8\n","permalink":"https://stefansiegl.de/2016/08/pig-latin-kata/","summary":"Yesterday I ran the Pig Latin kata at the local software craftsmenship meetup in Nuremberg. Picking Pig Latin as the kata to do was more a coincidence than planned, but it turned out to be an interesting choice.\nSo what I\u0026rsquo;ve prepared were four user stories (from which we only tackeled two; one team did three), going like this:\n(if you\u0026rsquo;d like to do the kata refrain from reading ahead and do one story after another)","title":"Pig Latin Kata"},{"content":"I\u0026rsquo;m currently very interested in serverless (aka no dedicated backend required) JavaScript Web Applications \u0026hellip; with AWS S3, Lambda \u0026amp; API Gateway you can actually get pretty far. Yet there is one thing I didn\u0026rsquo;t know how to do: Pub/Sub or \u0026ldquo;Realtime Messaging\u0026rdquo;.\nRealtime messaging allows to build web applications that can instantly receive messages published by another application (or the same one running in a different person\u0026rsquo;s browser). There even are cloud services permitting to do exactly this, e.g. Realtime Messaging Platform and PubNub Data Streams \u0026hellip;\nHowever recently having played with AWS Lambda and S3 I was wondering how this could be achieved on AWS\u0026hellip; and at first it seemed like it really isn\u0026rsquo;t possible. Especially the otherwise very interesting article Receiving AWS IoT messages in your browser using websockets by @jtparreira misled me, as he\u0026rsquo;s telling that it wouldn\u0026rsquo;t be possible. The article was published Nov 2015, \u0026hellip; not so long ago. But turns out it\u0026rsquo;s outdated anyways\u0026hellip;\nEnter AWS IoT While reading I stumbled over AWS IoT which allows to connect \u0026ldquo;Internet of Things\u0026rdquo; devices to the AWS cloud and furthermore provides messaging between those devices. It has a message broker (aka Device Gateway) sitting in the middle and \u0026ldquo;things\u0026rdquo; around it that connect to it. It\u0026rsquo;s based on the MQTT protocol and there are SDKs for the Raspberry Pi (Node.js), Android \u0026amp; iOS \u0026hellip; sound\u0026rsquo;s interesting, but not at all like \u0026ldquo;web browsers\u0026rdquo;\nMQTT over Web Sockets Then I found an announcement: AWS IoT Now Supports WebSockets published Jan 28, 2016. Brand new, but sounds great :)\n\u0026hellip; so even when IoT still sounds strange to do Pub/Sub with - it looks like a way to go.\nMaking it work For the proof of concept I didn\u0026rsquo;t care to publish AWS IAM User keys to the web application (of course this is a smell to be fixed before production use). So I went to \u0026ldquo;IAM\u0026rdquo; in the AWS management console and created a new user first, attaching the pre-defined AWSIoTDataAccess policy.\nSo the proof of concept should involve a simple web page that allows to establish a connection to the broker, features a text box where a message can be typed plus a publish button. So if two browsers are connected simultaneously then both should immediately receive messages published by one of them.\nrequired parts: \u0026hellip; we of course need a MQTT client and we need to do AWS-style request signing in the browser. NPM modules to the rescue:\naws-signature-v4 does the signature calculation\ncrypto helps it + some extra hashing we need to do\nmqtt has an MqttClient\n\u0026hellip; all of them have browser support through webpack. So we just need some more JavaScript to string everything together. To set up the connection:\nlet client = new MqttClient(() =\u0026gt; { const url = v4.createPresignedURL( \u0026#39;GET\u0026#39;, AWS_IOT_ENDPOINT_HOST.toLowerCase(), \u0026#39;/mqtt\u0026#39;, \u0026#39;iotdevicegateway\u0026#39;, crypto.createHash(\u0026#39;sha256\u0026#39;).update(\u0026#39;\u0026#39;, \u0026#39;utf8\u0026#39;).digest(\u0026#39;hex\u0026#39;), { \u0026#39;key\u0026#39;: AWS_ACCESS_KEY, \u0026#39;secret\u0026#39;: AWS_SECRET_ACCESS_KEY, \u0026#39;protocol\u0026#39;: \u0026#39;wss\u0026#39;, \u0026#39;expires\u0026#39;: 15 } ); return websocket(url, [ \u0026#39;mqttv3.1\u0026#39; ]); }); \u0026hellip; here createPresignedURL from aws-signature-v4 first does the heavy-lifting for us. We tell it the IoT endpoint address, protocol plus AWS credentials and it provides us with the signed URL to connect to.\nThere was just one stumbling block to me: I had upper-case letters in the hostname (as it is output by aws iot describe-endpoint command), the module however doesn\u0026rsquo;t convert these to lower case as expected by AWS\u0026rsquo; V4 signing process \u0026hellip; and as a matter of that access was denied first.\nHaving the signed URL we simply pass it on to a websocket-stream and create a new MqttClient instance around it.\nConnection established \u0026hellip; time to subscibe to a topic. Turns out to be simple:\nclient.on(\u0026#39;connect\u0026#39;, () =\u0026gt; client.subscribe(MQTT_TOPIC)); Handling incoming messages \u0026hellip; also easy:\nclient.on(\u0026#39;message\u0026#39;, (topic, message) =\u0026gt; console.log(message.toString())); \u0026hellip; and last not least publishing messages \u0026hellip; trivial again:\nclient.publish(MQTT_TOPIC, message); \u0026hellip; that\u0026rsquo;s it :-)\nMy proof of concept here\u0026rsquo;s what it looks like:\n\u0026hellip; the last incoming message was published from another browser running the exact same application.\nI\u0026rsquo;ve published my source code as a Gist on Github, feel free to re-use it.\nTo try it yourself:\nclone the Gist\nadjust the constants declared at the top of main.js as needed\ncreate a user in IAM first, see above\nfor the endpoint host run aws iot describe-endpoint CLI command\nrun npm install\nrun ./node_modules/.bin/webpack-dev-server --colors\nNext steps This was just the first (big) part. There\u0026rsquo;s more stuff left to be done:\nneither is hard-coding AWS credentials into the application source the way to go nor is publishing the secret key at all\n\u0026hellip; one possible approach would be to use the API Gateway + Lambda to create pre-signed URLs\n\u0026hellip; this could be further limited by using IAM roles and temporary identity federation (through STS Token Service)\nthere\u0026rsquo;s no user authentication yet, this should be achievable with AWS Cognito\n\u0026hellip; with that publishing/subscribing could be limitted to identity-related topics (depends on the use case)\n","permalink":"https://stefansiegl.de/2016/04/aws-iot-pubsub/","summary":"I\u0026rsquo;m currently very interested in serverless (aka no dedicated backend required) JavaScript Web Applications \u0026hellip; with AWS S3, Lambda \u0026amp; API Gateway you can actually get pretty far. Yet there is one thing I didn\u0026rsquo;t know how to do: Pub/Sub or \u0026ldquo;Realtime Messaging\u0026rdquo;.\nRealtime messaging allows to build web applications that can instantly receive messages published by another application (or the same one running in a different person\u0026rsquo;s browser). There even are cloud services permitting to do exactly this, e.","title":"Serverless Pub Sub with AWS IoT"},{"content":"Yesterday @dzuelke poked me to migrate the old PHP buildpack adjusted for V8Js to the new custom platform repo infrastructure. The advantage is that the custom platform repo only contains the v8js extension packages now, the rest (i.e. Apache and PHP itself) are pulled from the lang-php bucket, aka normal php buildpack.\nAs I already had that on my TODO list, I just immediately did that :-)\n\u0026hellip; so here\u0026rsquo;s the new heroku-v8js Github repository that has all the build formulas. Besides that there now is a S3 bucket heroku-v8js that stores the pre-compiled V8Js extensions for PHP 5.5, 5.6 and 7.0. packages.json file here.\nTo use with Heroku, just run\n$ heroku config:set HEROKU_PHP_PLATFORM_REPOSITORIES=\u0026#34;https://heroku-v8js.s3.amazonaws.com/dist-cedar-14-stable/packages.json\u0026#34; with Dokku:\n$ dokku config:set YOUR_APPNAME HEROKU_PHP_PLATFORM_REPOSITORIES=\u0026#34;https://heroku-v8js.s3.amazonaws.com/dist-cedar-14-stable/packages.json\u0026#34; ","permalink":"https://stefansiegl.de/2016/03/heroku-custom-platform-repo/","summary":"Yesterday @dzuelke poked me to migrate the old PHP buildpack adjusted for V8Js to the new custom platform repo infrastructure. The advantage is that the custom platform repo only contains the v8js extension packages now, the rest (i.e. Apache and PHP itself) are pulled from the lang-php bucket, aka normal php buildpack.\nAs I already had that on my TODO list, I just immediately did that :-)\n\u0026hellip; so here\u0026rsquo;s the new heroku-v8js Github repository that has all the build formulas.","title":"Heroku custom platform repo for V8Js"},{"content":"After fixing V8Js\u0026rsquo; behaviour of not retaining the object identity of passed back V8Object instances (i.e. always re-wrapping them, instead of re-using the already existing object) I tried how V8Js handles fluent setters (those that return $this at the end).\nUnfortunately they weren\u0026rsquo;t handled well, that is V8Js always wrapped the same object again and again (in both directions). Functionality-wise that doesn\u0026rsquo;t make a big difference since the underlying object is the same, hence further setters can still be called. But still the wrapping code takes some time \u0026ndash; with simple \u0026ldquo;just store that\u0026rdquo; setters it is approximately half of the time. Here is a performance comparison of calling 200000 minimalist fluent setters one after another:\nBesides the performance gain it also keeps object identity intact, however I assume noone ever stores the result of such a setter to a variable and compares it against another object. So that isn\u0026rsquo;t a big deal by itself.\nThe behaviour is changed with pull requests #220 and #221.\n","permalink":"https://stefansiegl.de/2016/03/fluent-setters/","summary":"After fixing V8Js\u0026rsquo; behaviour of not retaining the object identity of passed back V8Object instances (i.e. always re-wrapping them, instead of re-using the already existing object) I tried how V8Js handles fluent setters (those that return $this at the end).\nUnfortunately they weren\u0026rsquo;t handled well, that is V8Js always wrapped the same object again and again (in both directions). Functionality-wise that doesn\u0026rsquo;t make a big difference since the underlying object is the same, hence further setters can still be called.","title":"V8Js improved fluent setter performance"},{"content":"V8 has support for ES6 Promises and they make a clean JS-side API. So why not create promises from PHP, (later on) being resolved by PHP?\nV8Js doesn\u0026rsquo;t allow direct creation of JS objects from PHP-code, a little JS-side helper needs to be used. One possibility is this:\nclass V8PromiseFactory { private $v8; public function __construct(V8Js $v8) { $this-\u0026gt;v8 = $v8; } public function __invoke($executor) { $trampoline = $this-\u0026gt;v8-\u0026gt;executeString( \u0026#39;(function(executor) { return new Promise(executor); })\u0026#39;); return $trampoline($executor); } } \u0026hellip; it can be used to construct an API method that returns a Promise like this:\n$v8 = new V8Js(); $promiseFactory = new V8PromiseFactory($v8); $v8-\u0026gt;theApiCall = function() use ($promiseFactory) { return $promiseFactory(function($resolve, $reject) { // do something (maybe async) here, finally call $resolve or $reject $resolve(42); }); }; $v8-\u0026gt;executeString(\u0026#34; const p = PHP.theApiCall(); p.then(function(result) { var_dump(result); }); \u0026#34;); this code\ninitializes V8, V8Js and the V8PromiseFactory first\nthen attaches an API call named theApiCall, that uses $promiseFactory and passes it an executor that immediately resolves to the integer 42.\nthen executes some JavaScript code that uses the theApiCall function and attaches a then function that simply echos the value (42)\nV8PromiseFactory::__invoke should cache $trampoline if it is used to create a lot of promises.\nThis code requires V8Js with pull request #219 applied to function properly.\n","permalink":"https://stefansiegl.de/2016/03/v8promisefactory/","summary":"V8 has support for ES6 Promises and they make a clean JS-side API. So why not create promises from PHP, (later on) being resolved by PHP?\nV8Js doesn\u0026rsquo;t allow direct creation of JS objects from PHP-code, a little JS-side helper needs to be used. One possibility is this:\nclass V8PromiseFactory { private $v8; public function __construct(V8Js $v8) { $this-\u0026gt;v8 = $v8; } public function __invoke($executor) { $trampoline = $this-\u0026gt;v8-\u0026gt;executeString( \u0026#39;(function(executor) { return new Promise(executor); })\u0026#39;); return $trampoline($executor); } } \u0026hellip; it can be used to construct an API method that returns a Promise like this:","title":"V8PromiseFactory"},{"content":"As I’ve recently been poked whether I had used phpspec and I had to negate, today I finally gave it a try (doing the Bowling Kata) \u0026hellip;\nphpspec has some class and method templating built into it. If for example a test fails due to a missing function, it asks whether it should create one (that does nothing at all). This is nice but IMHO breaks the workflow a bit as you have to move the cursor to the terminal window and answer the question. You don’t just Shift+F10, see “red” in the panel and then hit Alt+Enter in PhpStorm and choose to create the method (which is my way of working with phpunit).\nI like the well readable test code that can be written with it like\n$this-\u0026gt;getScore()-\u0026gt;shouldReturn(150) \u0026hellip; yet that code shows also what I hate about it. Since $this actually is the test-class, having to call the message to test on it feels strange (or even wrong) and also phpstorm has no support for that \u0026hellip; so no auto-completion here.\nCalling methods of the SUT directly on $this gets even more messy once you add test helper methods like\nfunction it_grants_spare_bonus() { $this-\u0026gt;rollSpare(); $this-\u0026gt;roll(5); $this-\u0026gt;rollMany(17, 0); $this-\u0026gt;getScore()-\u0026gt;shouldBe(20); } \u0026hellip; here only roll is a method of the SUT, rollSpare and rollMany are just helper methods.\nAfter all I\u0026rsquo;m still torn, I like the readability, but the rest still feels strange and I miss native support in PhpStorm.\n","permalink":"https://stefansiegl.de/2016/03/thoughts-on-phpspec/","summary":"As I’ve recently been poked whether I had used phpspec and I had to negate, today I finally gave it a try (doing the Bowling Kata) \u0026hellip;\nphpspec has some class and method templating built into it. If for example a test fails due to a missing function, it asks whether it should create one (that does nothing at all). This is nice but IMHO breaks the workflow a bit as you have to move the cursor to the terminal window and answer the question.","title":"thoughts on phpspec"},{"content":"After @sD_Tobi recently poked me whether I knew phpspec, … and initially I had no idea except for having heard of it … I found this excellent screencast on laracasts.com to get a first impression. It\u0026rsquo;s just 17 minutes and he\u0026rsquo;s really enthusiastic about it :-)\n","permalink":"https://stefansiegl.de/2016/03/hello-phpspec/","summary":"After @sD_Tobi recently poked me whether I knew phpspec, … and initially I had no idea except for having heard of it … I found this excellent screencast on laracasts.com to get a first impression. It\u0026rsquo;s just 17 minutes and he\u0026rsquo;s really enthusiastic about it :-)","title":"phpspec screencast"},{"content":"The other day I paired with the guys from @solutiondrive and @niklas_heer, we had a fun evening learing about happy numbers, shared PhpStorm knowledge, tried Codeception etc. Actually we didn\u0026rsquo;t even finish the \u0026ldquo;Happy Numbers\u0026rdquo; Kata, since we only wrote the classifying routine, not the loop generating the output.\nOn my way home I kept googling and also found out about Lucky Numbers. Lucky numbers are natural numbers, recursively filtered by a sieve that eliminates numbers based on their position (where the second number tells the elimination offsets).\nSo I immediately came up with another Kata: generating those numbers. My constraint: no upper limit, i.e. use PHP\u0026rsquo;s Generator instead \u0026hellip; so I came up with the idea to implement the sieve itself as a Generator, that reads from an injected Generator, filters as needed and yields the result. The first \u0026ldquo;sieve generator\u0026rdquo; is fed from another generator that simply yields all natural numbers. The second one is fed from the first one and so on. The generator into generator injection is handled by yet another generator \u0026hellip; turn\u0026rsquo;s out: it works, but doesn\u0026rsquo;t look so nice. The outer generator cannot simply inject generators endlessly (since they are actually instanciated), so injection has to be deferred - that however dilutes the self-contained sieve generator :-(\nAnyways it was a good exercise on PHP\u0026rsquo;s generators. I think I\u0026rsquo;ll give it another try soon, again with generators yet another approach.\n","permalink":"https://stefansiegl.de/2016/03/happy-and-lucky-numbers/","summary":"The other day I paired with the guys from @solutiondrive and @niklas_heer, we had a fun evening learing about happy numbers, shared PhpStorm knowledge, tried Codeception etc. Actually we didn\u0026rsquo;t even finish the \u0026ldquo;Happy Numbers\u0026rdquo; Kata, since we only wrote the classifying routine, not the loop generating the output.\nOn my way home I kept googling and also found out about Lucky Numbers. Lucky numbers are natural numbers, recursively filtered by a sieve that eliminates numbers based on their position (where the second number tells the elimination offsets).","title":"Happy \u0026 Lucky Numbers"},{"content":"Playing around with V8\u0026rsquo;s custom startup snapshots I noticed some funny behaviour regarding Math.random.\nIt is clear that if you call Math.random() within the custom startup code the generated random numbers are baked into the snapshot and then not so random anymore. If you call Math.random() at runtime, without custom startup code, it just behaves as expected: it generates random numbers. However if you have custom startup code, calling Math.random() early on startup, it correctly generates random numbers during startup but it breaks runtime random number generation causing weird error messages like\nTypeError: Cannot read property \u0026#39;4\u0026#39; of undefined @virgofx raised this issue at the V8 issue tracker.\nFor the moment I came up with using random numbers from PHP\u0026rsquo;s Mersenne Twister\n$this-\u0026gt;v8 = new V8Js(\u0026#39;PHP\u0026#39;, [], [], true, $blob); $this-\u0026gt;v8-\u0026gt;__random = function() { return mt_rand() / mt_getrandmax(); }; $this-\u0026gt;v8-\u0026gt;executeString(\u0026#39;Math.random = PHP.__random; \u0026#39;); ","permalink":"https://stefansiegl.de/2016/03/math-random-fun/","summary":"Playing around with V8\u0026rsquo;s custom startup snapshots I noticed some funny behaviour regarding Math.random.\nIt is clear that if you call Math.random() within the custom startup code the generated random numbers are baked into the snapshot and then not so random anymore. If you call Math.random() at runtime, without custom startup code, it just behaves as expected: it generates random numbers. However if you have custom startup code, calling Math.random() early on startup, it correctly generates random numbers during startup but it breaks runtime random number generation causing weird error messages like","title":"funny Math.random behaviour"},{"content":"Recently @virgofx filed an issue on V8Js whether (startup) performance of V8Js could be increased. He wants to do server-side React rendering and noticed that V8 itself needs roughly 50ms to initialize and then further 60ms to process React \u0026amp; ReactServer javascript code. Way too much for server side rendering (on more or less every request).\nUp to V8 4.4 you simply could compile it with snapshots and V8Js made use of them. With 4.4 that stopped (V8Js just kept crashing), and I never really cared what they could do nor what the performance hit of this is, I just disabled them.\n\u0026hellip; turns out there even are three modes:\nno snapshots at all (what I did then)\nsnapshots support enabled, with external snapshot data (the default)\nsnapshots support enabled, with internal snapshot data (the snapshots are then linked into the library itself)\nThose snapshots are created once at compile time and store the state of V8\u0026rsquo;s heap after it has fully initialized itself. Hence their benefit is that the engine doesn\u0026rsquo;t fully bootstrap over and over, \u0026hellip; it simply restores the snapshot and is (almost) ready to start.\nOnly the second of those three modes wasn\u0026rsquo;t supported by V8Js, since it simply didn\u0026rsquo;t provide the external startup data \u0026ndash; and hence V8 failed to start.\nDigging deeper into snapshots I found out about custom startup snapshots. V8 since version 4.3 allows extra JavaScript code to be embedded into the snapshot itself. This is you can bake React \u0026amp; ReactServer right into the snapshot so it doesn\u0026rsquo;t have to re-evaluate the source over and over again.\nThe performance impact of this is enormous:\nThe Y-axis shows milliseconds, the blue bar the amount of time needed by V8 to bootstrap, the red bar time needed to evaluate React \u0026amp; ReactServer source code. Timings are averages over 100 samples taken on my Core i5 laptop.\nI compiled V8 5.0.104 with snapshot support, hence the blue bar immediately drops from about 60 ms down to about 4 ms. Since the base snapshots doesn\u0026rsquo;t have React included, the red bare remains at ~90 ms at first.\n\u0026hellip; including React into the snapshot, the red bar of course is gone, bootstrapping takes a little longer then \u0026ndash; but it is many times faster than without snapshots.\n","permalink":"https://stefansiegl.de/2016/02/snapshot-performance/","summary":"Recently @virgofx filed an issue on V8Js whether (startup) performance of V8Js could be increased. He wants to do server-side React rendering and noticed that V8 itself needs roughly 50ms to initialize and then further 60ms to process React \u0026amp; ReactServer javascript code. Way too much for server side rendering (on more or less every request).\nUp to V8 4.4 you simply could compile it with snapshots and V8Js made use of them.","title":"20x performance boost with V8Js snapshots"},{"content":"For some weeks now I had the idea that V8Js must be running on PHP7 the day it is officially published. So when I started out porting soon after they published he first release candidate (aka 7.0.0RC1) I felt some pressure, especially after noticing that it really will be a lot of work to do.\nThe more glad I am to announce today, that V8Js finally compiles fine and passes the whole test suite from the master branch (apart from tiny modifications that became necessary due to PHP 5.6 to PHP 7 incompatibilities).\nSince it works now, I\u0026rsquo;ve moved the \u0026ldquo;php7\u0026rdquo; branch from my personal repository to the official V8Js Github repository meanwhile. Jenkins already is prepared as well, among others it now has a PHP7 V8Js matrix job, that currently checks all release candidates in combination with some V8 versions, regularly, on every commit.\n","permalink":"https://stefansiegl.de/2015/10/v8js-on-php7/","summary":"For some weeks now I had the idea that V8Js must be running on PHP7 the day it is officially published. So when I started out porting soon after they published he first release candidate (aka 7.0.0RC1) I felt some pressure, especially after noticing that it really will be a lot of work to do.\nThe more glad I am to announce today, that V8Js finally compiles fine and passes the whole test suite from the master branch (apart from tiny modifications that became necessary due to PHP 5.","title":"V8Js approaching PHP7"},{"content":"Today as well as last Thursday I uploaded two more V8Js releases to PECL, both fixing issues around v8::FunctionTemplate usage that bit me at work.\nThose v8::FunctionTemplate objects are used to construct constructor functions (and thus object templates) in V8. The problem with them? They are not object to garbage collection. So if we export a object with a method attached to it from PHP to JS, V8Js at first exports the object (and caches the v8::FunctionTemplate used to construct it; re-using it on subsequent export of the same class). If JS code wants to call the method first the named property is got, which exports a function object (via a v8::FunctionTemplate) to the JavaScript world \u0026ndash; afterwards the function object is invoked. The problem: this v8::FunctionTemplate was not cached, hence re-created on each and every call of the method, of course leading to problems if functions are called thousands of times.\nVersion 0.2.4 fixes a related issue, regarding export of \u0026ldquo;normal\u0026rdquo; numeric arrays to JavaScript. Those are exported to Array-esque objects, that however do not share the normal Array prototype \u0026hellip; the template needed to construct those was, once more, not cached \u0026hellip; and hence lead to thousands of those templates lingering around unusable.\n","permalink":"https://stefansiegl.de/2015/09/two-more-releases/","summary":"Today as well as last Thursday I uploaded two more V8Js releases to PECL, both fixing issues around v8::FunctionTemplate usage that bit me at work.\nThose v8::FunctionTemplate objects are used to construct constructor functions (and thus object templates) in V8. The problem with them? They are not object to garbage collection. So if we export a object with a method attached to it from PHP to JS, V8Js at first exports the object (and caches the v8::FunctionTemplate used to construct it; re-using it on subsequent export of the same class).","title":"Two more V8Js releases"},{"content":"Today I noticed, that invocations of V8Function objects have a really poor call performance. A simple example might be:\n$v8 = new V8Js(); $func = $v8-\u0026gt;executeString(\u0026#39;(function() { print(\u0026#34;Hello\\\\n\u0026#34;); });\u0026#39;); for($i = 0; $i \u0026lt; 1000; $i ++) { $func(); } \u0026hellip; on my laptop this takes 2.466 seconds (with latest V8Js 0.2.1); older versions like V8Js 0.1.5 even take 80 seconds.\nThat felt strange, since V8Js performance generally is pretty good and the slightly changed version\n$v8 = new V8Js(); for($i = 0; $i \u0026lt; 1000; $i ++) { $v8-\u0026gt;executeString(\u0026#39;(function() { print(\u0026#34;Hello World\\\\n\u0026#34;); })();\u0026#39;); } \u0026hellip; has drastically better performance figures, just 0.168 seconds with recent V8Js and 0.247 seconds with ancient 0.1.5.\nSo there clearly is something going wrong.\nMy pull request #159 shows the solution, V8Js was re-using cached v8::Context on subsequent executeString calls but kept creating new v8::Context instances for V8Function invocations. With the patch applied the first example now passes in 0.135 seconds, which is slightly better than the executeString performance (as expected).\nAfter that huge improvement I released V8Js version 0.2.2, which also ships some memory leaks and errors mainly related to require() functionality.\n","permalink":"https://stefansiegl.de/2015/08/poor-v8js-function-call-performance/","summary":"Today I noticed, that invocations of V8Function objects have a really poor call performance. A simple example might be:\n$v8 = new V8Js(); $func = $v8-\u0026gt;executeString(\u0026#39;(function() { print(\u0026#34;Hello\\\\n\u0026#34;); });\u0026#39;); for($i = 0; $i \u0026lt; 1000; $i ++) { $func(); } \u0026hellip; on my laptop this takes 2.466 seconds (with latest V8Js 0.2.1); older versions like V8Js 0.1.5 even take 80 seconds.\nThat felt strange, since V8Js performance generally is pretty good and the slightly changed version","title":"Poor V8Function call performance"},{"content":"After waiting for a really long time (half a year) I finally have an approved php.net + PECL account granted with lead-rights on V8Js :-)\n\u0026hellip; therefore I finally published V8Js version 0.2.0, succeeding 0.1.5 which was published 1.5 years ago.\nChanges include\nadapt to latest v8 API (v8 versions from 3.24.6 up to latest 4.3 branch supported now)\nv8 debugging support\napply time \u0026amp; memory limits to V8Function calls\nsupport mapping of PHP objects implementing ArrayAccess to native arrays\nnew API to set limits: setTimeLimit \u0026amp; setMemoryLimit methods on V8Js object\ntypesafe JavaScript function wrappers\nimproved back-and-forth object passing (rewrapping, correcty isolate unlocking)\nfix property and method visibility issues\nfix memory leaks\nDownload the release from PECL repository.\n","permalink":"https://stefansiegl.de/2015/03/php-account-approved/","summary":"After waiting for a really long time (half a year) I finally have an approved php.net + PECL account granted with lead-rights on V8Js :-)\n\u0026hellip; therefore I finally published V8Js version 0.2.0, succeeding 0.1.5 which was published 1.5 years ago.\nChanges include\nadapt to latest v8 API (v8 versions from 3.24.6 up to latest 4.3 branch supported now)\nv8 debugging support\napply time \u0026amp; memory limits to V8Function calls","title":"php.net account approved"},{"content":"Today the last pull request in a series of contributions to the V8Js PHP extension has been merged. Good time to loose some words on the project and why I like it, so here we go :-)\nV8Js is a PHP extension that integrates Google\u0026rsquo;s V8 JavaScript engine into PHP. This is the extension allows you to execute JavaScript code securely sandboxed from PHP. Besides it allows for simple exchange of data from PHP to JavaScript and back.\nI like V8Js as it allows to run customer-provided code on the server, knowing that it is properly sandboxed so it cannot interact with all your PHP classes, variables and whatnot. Instead you can (and have to) provide a restricted set of classes acting as an API the JavaScript code can use.\nFirst things first, a simple hello world:\n$a = new V8Js(); $a-\u0026gt;executeString(\u0026#39;print(\u0026#34;Hello World\\n\u0026#34;);\u0026#39;); \u0026hellip; super simple and doesn\u0026rsquo;t do very much.\nOf course you can inject object instances as well:\nclass LoaderWriter { public function addRecord($type = \u0026#39;text\u0026#39;, $value = \u0026#39;\u0026#39;) { echo \u0026#34;addRecord -- $type, $value\\n\u0026#34;; } } $jscode = \u0026lt;\u0026lt;\u0026lt; EOT PHP.loader.addRecord(\u0026#34;text\u0026#34;, \u0026#34;first loader insert\u0026#34;); EOT; $a = new V8Js(); $a-\u0026gt;loader = new LoaderWriter(); $a-\u0026gt;executeString($jscode); However that\u0026rsquo;s still stuff you\u0026rsquo;d expect to work. What about pushing closures from JavaScript to PHP and call these from PHP? Works!\nclass Parser { protected $_callbacks = array(); public function on($element, $callback) { $this-\u0026gt;_callbacks[$element] = $callback; } public function runParser() { // should be fleshed out of course :-) $this-\u0026gt;_callbacks[\u0026#39;node\u0026#39;](\u0026#39;node-1 content\u0026#39;); $this-\u0026gt;_callbacks[\u0026#39;node\u0026#39;](\u0026#39;node-2 content\u0026#39;); } } $jscode = \u0026lt;\u0026lt;\u0026lt; EOT PHP.parser.on(\u0026#34;node\u0026#34;, function(data) { print(\u0026#34;Found node, content \u0026#34; + data + \u0026#34;\\n\u0026#34;); }); EOT; $a = new V8Js(); $a-\u0026gt;parser = new Parser(); $a-\u0026gt;executeString($jscode); $a-\u0026gt;parser-\u0026gt;runParser(); \u0026hellip; this way you can easily use PHP\u0026rsquo;s nice XmlReader to read chunks from XML files and have a customer-provided piece of JavaScript code bind on certain elements to drive customer-fitted data imports.\nThe code just does what you expect, it initializes a parser class, binds an element handle (which is a JavaScript function) and afterwards the parser (written in PHP) just calls the callback function transparently. And of course it\u0026rsquo;s not just possible to provide scalar values back and forth, you can pass objects just as transparently.\nTurned out that V8Js leaked some memory for each object being passed back to JavaScript, which doesn\u0026rsquo;t hurt much if you do it for a limited number of times. If you run the callback from above for a huge XML file you\u0026rsquo;re quickly hit. The problem was, that V8Js incremented the refcount on the PHP object, but didn\u0026rsquo;t decrease if V8 decided to dispose the JavaScript instance. The fix for this is merged since July 11th.\nOfficially the PECL package is still in beta state and I found some bugs after a limited number of evaluation hours \u0026hellip; if you can live with that I think V8Js is a pretty cool sandbox environment, definitely worth a look.\nSince I\u0026rsquo;ve made myself familiar with the source I also replaced deprecated calls to V8 API by newer equivalents and allowed for construction of PHP objects from JavaScript. This is do stuff like that:\n$v8 = new V8Js(); class Greeter { function sayHello($a) { echo \u0026#34;Hello $a\\n\u0026#34;; } } $jscode = \u0026lt;\u0026lt;\u0026lt; EOT PHP.greeter.sayHello(\u0026#34;John\u0026#34;); // prints \u0026#34;Hello John\u0026#34; as expected print(PHP.greeter); print(\u0026#34;\\n\u0026#34;); // prints \u0026#34;[object Greeter]\u0026#34; as expected // What about the constructor function of greeter? print(PHP.greeter.constructor); // ... yields \u0026#34;function Greeter() { [native code] }\u0026#34; // ... super, so let me create more greeters :-) var ngGreeter = new PHP.greeter.constructor(); ngGreeter.sayHello(\u0026#34;Ringo\u0026#34;); // well, ... used to segfault :-) EOT; $v8-\u0026gt;greeter = new Greeter(); $v8-\u0026gt;executeString($jscode); \u0026hellip; I can\u0026rsquo;t immediately come up with a use case, but hey, JavaScript is all about (constructor) functions, so it definitely should work. And of course it shouldn\u0026rsquo;t be possible to crash the sandbox :-)\n","permalink":"https://stefansiegl.de/2013/07/v8js-patches-merged/","summary":"Today the last pull request in a series of contributions to the V8Js PHP extension has been merged. Good time to loose some words on the project and why I like it, so here we go :-)\nV8Js is a PHP extension that integrates Google\u0026rsquo;s V8 JavaScript engine into PHP. This is the extension allows you to execute JavaScript code securely sandboxed from PHP. Besides it allows for simple exchange of data from PHP to JavaScript and back.","title":"V8Js Patches merged"},{"content":"Anschrift Stefan Siegl\nAnnastraße 17a\n97072 Würzburg\nKontakt E-Mail: mail(at)stefansiegl.de GnuPG pub rsa4096 2017-01-16 [SC] [expires: 2026-01-06] F88D 9416 3B2D B0AA FB57 5C79 5157 5950 1548 39CD uid [ultimate] Stefan Siegl \u0026lt;stesie@brokenpipe.de\u0026gt; uid [ultimate] Stefan Siegl \u0026lt;stesie@php.net\u0026gt; https://keyserver.ubuntu.com/pks/lookup?op=get\u0026amp;search=0xf88d94163b2db0aafb575c7951575950154839cd ","permalink":"https://stefansiegl.de/imprint/","summary":"imprint","title":"Imprint"}]